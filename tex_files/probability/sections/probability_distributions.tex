\documentclass[../probability-notes.tex]{subfiles}
\begin{document}
    \section{Binomial Random Variable}
    \emph{Binomial Random Variable} $X$ is defined as the number of successes in an experiment with $n$ independent trials, where each trial can only have two outcomes, \emph{success} or \emph{failure}.\\
    Let $X_{i}$ denote the Random Variable corresponding to the individual trials, with probability of success $p$. Then we have the following

    \begin{alignat*}{2}
        X_{i} &= \begin{cases} 1 &\mbox{if success in trial i}\\ 
                                0 &\mbox{otherwise} \end{cases} \tag*{indicator variable} \\
        X &= X_{1} + X_{2} + \cdots + X_{n} = \sum_{i=1}^{n} X_{i} \\
        P(X=k) &= \sum_{k=0}^{n} \binom{n}{k} p^{k} (1 - p)^{n-k}
    \end{alignat*}

    \subsection{Mean and Variance}
    First let's calculate the mean and variance for a single trial $X_{i}$
    \begin{alignat*}{2}
        E[X_{i}] &= 1 * p + 0 * (1 - p) &&= p\\
        Var(X_{i}) &= (1 - p)^{2}p + (0-p)^{2}(1-p) &&= p(1-p)
    \end{alignat*}
    
    We know that all $X_{i}'s$ are independent. Hence, the mean and variance for X become
    \begin{alignat*}{3}
        E[X] &= E[\sum_{i=1}^{n} X_{i}] &&= \sum_{i=1}^{n}E[X_{i}] &&= np \\
        Var(X) &= Var(\sum_{i=1}^{n} X_{i}) &&= \sum_{i=1}^{n} Var(X_{i}) &&= np(1-p)
    \end{alignat*}

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Continuous Uniform Random Variable}
    A uniform random variable is defined as follows
    \begin{align*}
        f_{X}(x) = \begin{cases} \frac{1}{b-a} &\mbox{$if a \leq x \leq b$}\\
                                    0 &\mbox{otherwise} \end{cases}
    \end{align*}

    \subsection{Mean and Variance}
    \begin{align*}
        E[X] &= \int_{a}^{b} x \frac{1}{b-a} dx = [\frac{x^{2}}{2(b-a)}]_{a}^{b}\\
            &= \frac{a+b}{2}\\
        Var(X) &= \int_{a}^{b} (x - \frac{a+b}{2})^{2} \frac{1}{b-a} dx \\
            &= \frac{(b-a)^{2}}{12}
    \end{align*}

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Gaussian Distribution}
    The gaussian distribution (or normal distribution) is defined between $-\inf$ and $\inf$. It is parametrized by mean $\mu$ and variance $\sigma$, $X \sim \mathcal{N}(\mu, \sigma^{2})$
    \begin{align*}
        f_{X}(x) = \frac{1}{\sqrt{2\pi \sigma^{2}}} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}}
    \end{align*}
    As already described,
    \begin{align*}
        E[X] &= \mu\\
        Var(X) &= \sigma^{2}
    \end{align*}

    A \emph{Standard Normal} is defined as a normal distribution with $\mu = 0$ and $\sigma^{2} = 1$\\
    Any normal distribution can be converted to a standard normal as $X = \frac{X - \mu}{\sigma}$\\
    If $Y = aX + b$, then $Y \sim \mathcal{N}(a \mu + b, a^{2}\sigma^{2})$

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Counting Process}
    Counting process is used in scenarios when we want to count the occurrence of a certain event. $N_{t}$ denotes the number of events till time $t$ starting from 0. It is assumed that $N_{0} = 0$. Formal definition is\newline

    A random process $\{N_{t}, t \in [0, \inf)\}$ is said to be a counting process if $N_{t}$ is the number of events from time $t=0$ upto time $t$. For a counting process, we assume
    \begin{enumerate}
        \item $N_{0} = 0$
        \item $N_{t} \in \{0, 1, 2, \cdots\}$ for all $t \in [0, \inf)$
        \item for $0 \leq s < t, N_{t} - N_{s} $ shows the number of events that occur in the interval $(s,t]$ 
    \end{enumerate}

    \subsection{Independent Increments}
    We say that a continuous time counting process $N_{t}$ has independent increments if for all $0 \leq t_{1} < t_{2} < \cdots < t_{n}$, the random variables
    \begin{align*}
         N_{t_{2}} - N_{t_{1}}, \;N_{t_{3}} - N_{t_{2}}, \ldots, \;N_{t_{n}} - N_{t_{n-1}}
    \end{align*}
    are independent.\newline

    Note that these differences are nothing but the number of arrivals in a given time interval. Thus, we are equivalently saying that \textbf{the number of arrivals in any two disjoint intervals are independent}.\newline

    A very simple consequence of this property is:\newline
    Suppose we wise to find the probability of 2 arrivals in the interval $(1,2]$ and 3 arrivals in the interval $(3,5]$. Then,
    \begin{align*}
        P(2 \text{ arrivals in } (1,2] \text{ and } 3 \text{ arrivals in } (3,5]) = P(2 \text{ arrivals in } (1,2]) P(3 \text{ arrivals in } (3,5]))
    \end{align*}
    since the arrivals in disjoint intervals are independent.

    \subsection{Stationary Increments}
    We say that a continuous time counting process $N_{t}$ has stationary increments if for all $t_{2} > t_{1} \geq 0$ and for all $r > 0$, $N_{t_{2}} - N_{t_{1}}$ and $N_{t_{2} + r}$ and $N_{t_{1} + r}$ are independent.\newline

    In other words, \textbf{the number of arrivals in a given time interval is invariant to it's location}. Note that the number of arrivals in the time interval between $t_{1}$ and $t_{2}$ is nothing but $N_{t_{2}} - N_{t_{1}}$. By the above statement, if the process has stationary increments, then this quantity is same as $N_{t_{2} - t_{1}}$, which is the distribution of the counting process itself.


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Renewal Process}
    \subfile{renewal_process}

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Bernoulli Process}
    Bernoulli process falls under the family of random processes, which are random variables continuously evolving over time. Bernoulli process can be described as a sequence of independent Bernoulli trials, where each trial has only two outcomes : success with $P(success) = p$ and failure.
    \begin{alignat*}{2}
        P_{X_{t}}(x_{t}) &= \begin{cases} p &\mbox{if $X_{t} = 1$}\\
                                        1-p &\mbox{if $X_{t} = 0$} \end{cases}\\
        E[X_{t}] &= p\\
        Var(X_{t}) &= p(1-p)
    \end{alignat*}

    \subsection{Mean and Variance}
    Number of successes S in n time slots
    \begin{align*}
        P(S=k) &= \binom{n}{k} p^{k}(1-p)^{n-k}\\
        E[S] &= np\\
        Var(S) &= np(1-p)
    \end{align*}

    \subsection{Interarrival Times (Geometric Random Variable)}
    Let $T_{1}$ denote the number of trials till the first success
    \begin{align*}
        P(T_{1} = t) &= (1-p)^{t-1}p \tag*{$t \in {1, 2, \ldots}$}\\
        E[T_{1}] &= \frac{1}{p}\\
        Var(T_{1}) &= \frac{1-p}{p^{2}}
    \end{align*}
    This process is memoryless as all future coin flips are independent of whatever has happened till now. Also, the distribution is a \textbf{Geometric Random Variable}.

    \subsection{Sum of Interarrival times}
    We are interested in the total time till k arrivals. Let this random variable be $Y_{k}$
    \begin{align*}
        Y_{k} &= T_{1} + T_{2} + \cdots + T_{k} \tag*{where $T_{i}$'s are i.i.d geometric with parameter $p$}\\
        P(Y_{k} = t) &= P(\text{$k-1$ arrivals between $t=1$ to $t=t$ and last arrival at time $t$})\\
           &= \binom{t-1}{k-1}p^{k}(1-p)^{t-k} \tag*{$\forall\; t \geq k$}\\
        E[Y_{k}] &= \sum_{i=1}{k}E[T_{i}]\\
                &= \frac{k}{p}\\
        Var(Y_{k}) &= \sum_{i=1}^{k}Var(T_{i})\\
                    &= \frac{k(1-p)}{p^{2}}
    \end{align*}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Exponential Distribution}
    Exponential distribution is characterized by the parameter $\lambda$ and has the following probability distribution
    \begin{align*}
        f_{X}(x) = \begin{cases} 0 &\mbox{if $x < 0$}\\
                                \lambda e^{-\lambda x} &\mbox{otherwise} \end{cases}
    \end{align*}

    Exponential distribution is used to represent the interarrival time probability distribution in the context of Poisson Process. The cumulative distribution is given by
    \begin{alignat*}{2}
        F_{X}(x) &= \begin{cases} 0 &\mbox{if $x < 0$}\\
                                1 - e^{-\lambda x} &\mbox{otherwise} \end{cases}\\
        P(X > x) &= \int_{x}^{\inf} \lambda e^{-\lambda x} dx\\
        &= e^{-\lambda x}
    \end{alignat*}

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsection{Mean and Variance}
    The mean of the distribution is given by
    \begin{align*}
        E[x] &= \int_{0}^{\inf} \lambda x e^{-\lambda x} dx\\
        &= [-x e^{-\lambda x}]_{0}^{\inf} + \int_{0}^{\inf} e^{-\lambda x} dx = \frac{1}{\lambda}\\
        \Aboxed{E[X] &= \frac{1}{\lambda}}
    \end{align*}
    where we used integration by parts, $\int uv' = uv - \int u'v$ and substituted $u = x$ and $v = -e^{-\lambda x}/\lambda$.\newline

    For variance, we first calculate the value of $E[x^{2}]$
    \begin{align*}
        E[x^{2}] &= \int_{0}^{\inf} \lambda x^{2} e^{-\lambda x} dx\\
        &= [-x^{2} e^{-\lambda x}]_{0}^{\inf} + \int_{0}^{\inf} 2x e^{-\lambda x} dx\\
        &= [\frac{-2x e^{-\lambda x}}{\lambda}]_{0}^{\inf} - [\frac{2e^{-\lambda x}}{\lambda^{2}}]_{0}^{\inf}\\
        &= \frac{2}{\lambda^{2}}\\
        Var(X) &= E[X^{2}] - E[X]^{2}\\
        \Aboxed{Var(X) &= \frac{1}{\lambda^{2}}}
    \end{align*}
    The above property can be generalized for the $n$th power as well
    \begin{align*}
        E[X^{n}] = \frac{n!}{\lambda^{n}}
    \end{align*}

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsection{Memoryless Property}
    A fundamental mathematical property of the exponential distribution is the memoryless property. In summary, this means that whatever has transpired till now will not affect the future distribution. Mathematically $P(T > t+s) is independent of t$
    \begin{align*}
        P(T > t+s | T>t) &= \frac{P(T> t+s \text{ and }T > t)}{P(T > t)}\\
        &= \frac{P(T > t + s)}{P(T > t)}\\
        &= \frac{e^{-\lambda(t+s)}}{e^{-\lambda t}}\\
        &= e^{-\lambda s}\\
        \Aboxed{P(T > t+s | T>t) &= P(T > s)}
    \end{align*}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Poisson Process}
    \subfile{poisson}

\end{document}