\documentclass[../probability-notes.tex]{subfiles}
\begin{document}
    \section{Binomial Random Variable}
    \emph{Binomial Random Variable} $X$ is defined as the number of successes in an experiment with $n$ independent trials, where each trial can only have two outcomes, \emph{success} or \emph{failure}.\\
    Let $X_{i}$ denote the Random Variable corresponding to the individual trials, with probability of success $p$. Then we have the following

    \begin{alignat*}{2}
        X_{i} &= \begin{cases} 1 &\mbox{if success in trial i}\\ 
                                0 &\mbox{otherwise} \end{cases} \tag*{indicator variable} \\
        X &= X_{1} + X_{2} + \cdots + X_{n} = \sum_{i=1}^{n} X_{i} \\
        P(X=k) &= \sum_{k=0}^{n} \binom{n}{k} p^{k} (1 - p)^{n-k}
    \end{alignat*}

    \subsection{Mean and Variance}
    First let's calculate the mean and variance for a single trial $X_{i}$
    \begin{alignat*}{2}
        E[X_{i}] &= 1 * p + 0 * (1 - p) &&= p\\
        Var(X_{i}) &= (1 - p)^{2}p + (0-p)^{2}(1-p) &&= p(1-p)
    \end{alignat*}
    
    We know that all $X_{i}'s$ are independent. Hence, the mean and variance for X become
    \begin{alignat*}{3}
        E[X] &= E[\sum_{i=1}^{n} X_{i}] &&= \sum_{i=1}^{n}E[X_{i}] &&= np \\
        Var(X) &= Var(\sum_{i=1}^{n} X_{i}) &&= \sum_{i=1}^{n} Var(X_{i}) &&= np(1-p)
    \end{alignat*}

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Continuous Uniform Random Variable}
    A uniform random variable is defined as follows
    \begin{align*}
        f_{X}(x) = \begin{cases} \frac{1}{b-a} &\mbox{$if a \leq x \leq b$}\\
                                    0 &\mbox{otherwise} \end{cases}
    \end{align*}

    \subsection{Mean and Variance}
    \begin{align*}
        E[X] &= \int_{a}^{b} x \frac{1}{b-a} dx = [\frac{x^{2}}{2(b-a)}]_{a}^{b}\\
            &= \frac{a+b}{2}\\
        Var(X) &= \int_{a}^{b} (x - \frac{a+b}{2})^{2} \frac{1}{b-a} dx \\
            &= \frac{(b-a)^{2}}{12}
    \end{align*}

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Gaussian Distribution}
    The gaussian distribution (or normal distribution) is defined between $-\inf$ and $\inf$. It is parametrized by mean $\mu$ and variance $\sigma$, $X \sim \mathcal{N}(\mu, \sigma^{2})$
    \begin{align*}
        f_{X}(x) = \frac{1}{\sqrt{2\pi \sigma^{2}}} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}}
    \end{align*}
    As already described,
    \begin{align*}
        E[X] &= \mu\\
        Var(X) &= \sigma^{2}
    \end{align*}

    A \emph{Standard Normal} is defined as a normal distribution with $\mu = 0$ and $\sigma^{2} = 1$\\
    Any normal distribution can be converted to a standard normal as $X = \frac{X - \mu}{\sigma}$\\
    If $Y = aX + b$, then $Y \sim \mathcal{N}(a \mu + b, a^{2}\sigma^{2})$

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Renewal Process}
    This is a fundamental stochastic process useful in modelling arrivals and interarrival times. Some definitions will make the usage clear.\newline

    Let $S_{i}$ denote the $i$th renewal time or the time when the $i$th arrival takes place. By definition, $S_{0} = 0$. We can also define
    \begin{align*}
        S_{n} &= S_{n-1} + \xi_{n}\\
        S_{n} &= \xi_{1} + \xi_{2} + \cdots + \xi_{n-1}
    \end{align*}
    where $\xi_{i}$ are positive ($P(\xi > 0) = 1$) independent identically distributed variables representing the interarrival times. We also define
    \begin{align*}
        N_{t} &= \argmax_{k} \{ S_{k} \leq t \}\\
        \{ S_{n} > t \} &= \{ N_{t} < n \}
    \end{align*}
    or, $N_{t}$ is simply the number of arrivals till the time $t$.\newline

    Define the following quantity
    \begin{align*}
        F^{n*} &= F_{\xi} * \ldots * F_{\xi} \text{ $n$ times}\\
        u(t) &= \sum_{i=1}^{\inf} F^{n*}(t)
    \end{align*}
    It can be shown that the function $u(t)$ converges. The expectation of $N_{t}$ then becomes
    \begin{alignat*}{2}
        E[N_{t}] &= E[\text{number of $n$ such that $S_{n} \leq t$}]\\
        &= E[\sum_{n=1}^{\inf} I(S_{n} \leq t)] &\text{ sum of Indicators will equal $n$}\\
        &= \sum_{n=1}^{\inf} P(S_{n} \leq t) &\text{ since $E[$Indicator$]$ is just the function inside indicator}\\
        &= \sum_{n=1}^{\inf} F^{n*}(t) &\text{ by defining cumulative as sum of $\xi$s}\\
        &= u(t)
    \end{alignat*}

    \subsection{Laplace Transform}
    For a density function $f$ defined from $\mathbb{R}^{\geq 0} \to \mathbb{R}$, Laplace transform is
    \begin{align*}
        L_{f}(s) = \int_{\mathbb{R}^{\geq 0}} e^{-sx} f(x) dx
    \end{align*}
    The following properties hold for this transform
    \begin{enumerate}
        \item If $f$ is a probability density function, then
        \begin{align*}
            E[e^{-sx}] = L_{f}(s)
        \end{align*}
        
        \item if $f_{1}$ and $f_{2}$ are two probability density functions, then
        \begin{align*}
            L_{f_{1}*f_{2}}(s) = L_{f_{1}}(s) L_{f_{2}}(s)
        \end{align*}
        
        \item \label{itm:laplace3} If $F$ is the cumulative probability distribution for $X$ and $p$ is the probability density function, then
        \begin{align*}
            L_{F_{X}}(s) = \frac{L_{p_{X}}(s)}{s}
        \end{align*}
        which can be proven using integration by parts as follows
        \begin{align*}
            L_{F_{X}}(s) = \int_{\mathbb{R}^{\geq 0}} F_{X}(x) \frac{d(e(-sx))}{s} = 0 + \frac{1}{s} \int_{\mathbb{R}^{\geq 0}} p_{X}(x) e^{-sx} dx
        \end{align*}
    \end{enumerate}

    \subsection{Calculating the Expectation}
    Armed with the concept of a Laplace transform, we make the following observation first
    \begin{align*}
        u(t) &= \sum_{i=1}^{\inf} F^{n*}(t) = F(t) + \sum_{i=2}^{\inf} F^{n*}(t)\\
        &= F(t) + \big( \sum_{i=1}^{\inf} F^{n*}(t) \big) * F(t)\\
        &= F(t) + u(t) * F(t)\\
       u(t) &= F(t) + u(t) * p(t)
    \end{align*}
    where $p$ is the probability density function and the last line stems from the fact that $\int u * F = \int u(x-y) dF(y) = \int u(x-y) p(y) dy$. Taking Laplace transform on both sides,

    \begin{align*}
        L_{u}(s) &= L_{F}(s) + L_{u}(s) L_{p}(s)\\
        L_{u}(s) &= \frac{L_{p}(s)}{s} + L_{u}(s) L_{p}(s) \text{ from \ref{itm:laplace3}}\\
        L_{u(s)} &= \frac{L_{p}(s)}{s(1-L_{p}(s))}
    \end{align*}

    The last equation can be used to calculate the laplace transform of $u(t)$ and consecutively guess the functional form of $u(t)$.

    \subsection{Limit Theorems for Renewal Processes}
    The following two theorems hold true for Renewal processes
    \begin{enumerate}
        \item If $E[\xi] = \mu < \inf$, then
        \begin{align*}
            \lim_{t \to \inf} \frac{N_{t}}{t} = \frac{1}{\mu}            
        \end{align*}
        which is analogous to the strong law of large numbers. This can be proven as follows
        \begin{align*}
            S_{N_{t}} \leq t \leq S_{N_{t} + 1} \text{ from the definition of $N_{t}$}\\
            \text{or, } \frac{N_{t}}{S_{N_{t} + 1}} \leq \frac{N_{t}}{t} \leq \frac{N_{t}}{S_{N_{t}}}
        \end{align*}
        we can calculate the limits on the two bounds as
        \begin{align*}
            \lim_{t \to \inf} \frac{N_{t}}{S_{N_{t}}} = \lim_{n \to \inf} \frac{n}{S_{n}} = \frac{1}{\mu}
        \end{align*}
        from the strong law of large numbers applied to $\lim_{n \to \inf} \frac{S_{n}}{n}$. Similarly, one can show
        \begin{align*}
            \lim_{t \to \inf} \frac{N_{t}}{S_{N_{t} + 1}} = \lim_{t \to \inf} \frac{N_{t}}{N_{t} + 1} \lim_{t \to \inf} \frac{N_{t} + 1}{S_{N_{t} + 1}} = 1 * \frac{1}{\mu}
        \end{align*}

        \item If $Var(\xi) = \sigma^{2} < \inf$, then
        \begin{align*}
            \lim_{t \to \inf} \frac{N_{t} - t/\mu}{\sigma \sqrt{t}/\mu^{3/2}} = \mathcal{N}(0,1)
        \end{align*}
        which is analogous to the central limit theorem. It can be proven by considering the CLT on $\xi$s
        \begin{align*}
            \lim_{n \to \inf} P(\frac{S_{n} - n\mu}{\sigma \sqrt{n}} \leq x) &= \text{CDF of }\mathcal{N}(0,1)\\
            \text{or, } \lim_{n \to \inf} P(S_{n} \leq n\mu + \sigma \sqrt{n} x) &= \text{CDF of }\mathcal{N}(0,1)\\
            \text{or, } \lim_{n \to \inf} P(N_{t} \geq n) &= \text{CDF of }\mathcal{N}(0,1) \text{ from definition of $N_{t}$, where $t = n\mu + \sigma \sqrt{n} x$}\\
        \end{align*}
        We substitute $n\mu = t$ for very large value of $n$, since the total time will become total variables into the expected time for one $\xi$ when $n$ is large. Hence,
        \begin{gather*}
            n = \frac{t}{\mu} - \frac{\sigma \sqrt{t}}{\mu^{3/2}}x\\
            \lim_{n \to \inf} P(N_{t} \geq n) = \lim_{n \to \inf} P(\frac{N_{t} - t/\mu}{\sigma \sqrt{t}/\mu^{3/2}} \leq x) = \text{CDF of }\mathcal{N}(0,1)
        \end{gather*}
    \end{enumerate}

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Bernoulli Process}
    Bernoulli process falls under the family of random processes, which are random variables continuously evolving over time. Bernoulli process can be described as a sequence of independent Bernoulli trials, where each trial has only two outcomes : success with $P(success) = p$ and failure.
    \begin{alignat*}{2}
        P_{X_{t}}(x_{t}) &= \begin{cases} p &\mbox{if $X_{t} = 1$}\\
                                        1-p &\mbox{if $X_{t} = 0$} \end{cases}\\
        E[X_{t}] &= p\\
        Var(X_{t}) &= p(1-p)
    \end{alignat*}

    \subsection{Mean and Variance}
    Number of successes S in n time slots
    \begin{align*}
        P(S=k) &= \binom{n}{k} p^{k}(1-p)^{n-k}\\
        E[S] &= np\\
        Var(S) &= np(1-p)
    \end{align*}

    \subsection{Interarrival Times (Geometric Random Variable)}
    Let $T_{1}$ denote the number of trials till the first success
    \begin{align*}
        P(T_{1} = t) &= (1-p)^{t-1}p \tag*{$t \in {1, 2, \ldots}$}\\
        E[T_{1}] &= \frac{1}{p}\\
        Var(T_{1}) &= \frac{1-p}{p^{2}}
    \end{align*}
    This process is memoryless as all future coin flips are independent of whatever has happened till now. Also, the distribution is a \textbf{Geometric Random Variable}.

    \subsection{Sum of Interarrival times}
    We are interested in the total time till k arrivals. Let this random variable be $Y_{k}$
    \begin{align*}
        Y_{k} &= T_{1} + T_{2} + \cdots + T_{k} \tag*{where $T_{i}$'s are i.i.d geometric with parameter $p$}\\
        P(Y_{k} = t) &= P(\text{$k-1$ arrivals between $t=1$ to $t=t$ and last arrival at time $t$})\\
           &= \binom{t-1}{k-1}p^{k}(1-p)^{t-k} \tag*{$\forall\; t \geq k$}\\
        E[Y_{k}] &= \sum_{i=1}{k}E[T_{i}]\\
                &= \frac{k}{p}\\
        Var(Y_{k}) &= \sum_{i=1}^{k}Var(T_{i})\\
                    &= \frac{k(1-p)}{p^{2}}
    \end{align*}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Exponential Distribution}
    Exponential distribution is characterized by the parameter $\lambda$ and has the following probability distribution
    \begin{align*}
        f_{X}(x) = \begin{cases} 0 &\mbox{if $x < 0$}\\
                                \lambda e^{-\lambda x} &\mbox{otherwise} \end{cases}
    \end{align*}

    Exponential distribution is used to represent the interarrival time probability distribution in the context of Poisson Process. The cumulative distribution is given by
    \begin{alignat*}{2}
        F_{X}(x) &= \begin{cases} 0 &\mbox{if $x < 0$}\\
                                1 - e^{-\lambda x} &\mbox{otherwise} \end{cases}\\
        P(X > x) &= \int_{x}^{\inf} \lambda e^{-\lambda x} dx\\
        &= e^{-\lambda x}
    \end{alignat*}

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsection{Mean and Variance}
    The mean of the distribution is given by
    \begin{align*}
        E[x] &= \int_{0}^{\inf} \lambda x e^{-\lambda x} dx\\
        &= [-x e^{-\lambda x}]_{0}^{\inf} + \int_{0}^{\inf} e^{-\lambda x} dx = \frac{1}{\lambda}\\
        \Aboxed{E[X] &= \frac{1}{\lambda}}
    \end{align*}
    where we used integration by parts, $\int uv' = uv - \int u'v$ and substituted $u = x$ and $v = -e^{-\lambda x}/\lambda$.\newline

    For variance, we first calculate the value of $E[x^{2}]$
    \begin{align*}
        E[x^{2}] &= \int_{0}^{\inf} \lambda x^{2} e^{-\lambda x} dx\\
        &= [-x^{2} e^{-\lambda x}]_{0}^{\inf} + \int_{0}^{\inf} 2x e^{-\lambda x} dx\\
        &= [\frac{-2x e^{-\lambda x}}{\lambda}]_{0}^{\inf} - [\frac{2e^{-\lambda x}}{\lambda^{2}}]_{0}^{\inf}\\
        &= \frac{2}{\lambda^{2}}\\
        Var(X) &= E[X^{2}] - E[X]^{2}\\
        \Aboxed{Var(X) &= \frac{1}{\lambda^{2}}}
    \end{align*}
    The above property can be generalized for the $n$th power as well
    \begin{align*}
        E[X^{n}] = \frac{n!}{\lambda^{n}}
    \end{align*}

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsection{Memoryless Property}
    A fundamental mathematical property of the exponential distribution is the memoryless property. In summary, this means that whatever has transpired till now will not affect the future distribution. Mathematically $P(T > t+s) is independent of t$
    \begin{align*}
        P(T > t+s | T>t) &= \frac{P(T> t+s \text{ and }T > t)}{P(T > t)}\\
        &= \frac{P(T > t + s)}{P(T > t)}\\
        &= \frac{e^{-\lambda(t+s)}}{e^{-\lambda t}}\\
        &= e^{-\lambda s}\\
        \Aboxed{P(T > t+s | T>t) &= P(T > s)}
    \end{align*}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Poisson Process}
    Poisson process also falls in the realm of random processes but is different from Bernoulli process as it is a continuous time process. This process is very commonly used to model arrival times and number of arrivals in a given time interval.
    \begin{align*}
        P(k, \tau) &= \text{Probability of $k$ arrivals in interval of duration $\tau$}\\
        \sum_{k} P(k, \tau) &= 1 \tag*{for a given $\tau$}
    \end{align*}
    Assumptions
    \begin{itemize}
        \item The Probability is dependent only on $\tau$ and not the \emph{location} of the interval
        \item Number of arrivals in disjoint time intervals are \emph{independent}
    \end{itemize}
    
    \subsection{Derivation from Bernoulli Process}
    For a very small interval $\delta$,
    \begin{alignat*}{2}
        P(k, \delta) &= \begin{cases} 1-\lambda \delta &\mbox{$k = 0$}\\
                                     \lambda \delta &\mbox{$k = 1$}\\
                                     0 &\mbox{$k > 2$} \end{cases} + O(\delta^{2})\\
        \lambda &= \lim_{\delta \to 0}\frac{P(1,\delta)}{\delta} \tag*{arrival rate per unit time}\\
        E[k] &= (\lambda \delta) * 1 + (1-\lambda \delta) * 0\\
            &= \lambda \delta \\
        \tau &= n \delta
    \end{alignat*}
    
    The last equation clearly implies that we can approximate the whole process as a bernoulli process where we have $n$ miniscule time intervals with at most one arrival per interval.
    \begin{align*}
        P(k\; arrivals) &= \binom{n}{k} p^{k} (1-p)^{n-k} \\
            &= \binom{n}{k} (\frac{\lambda \delta}{n})^{k} (1 - \frac{\lambda \delta}{n})^{n-k}\\
        \lambda \tau &= np \tag*{or, arrival rate * time = E[arrivals]}\\
        Poisson &= \lim_{\delta \to 0, n \to \inf} Bernoulli\\
        or,\; P(k, \tau) &= \frac{(\lambda \tau)^{k} e^{-\lambda \tau}}{k!} \tag*{$k = 0,1, \cdots$, for a given $\tau$}\\
        where,\; \sum_{k} P(k, \tau) &= 1 \tag*{for a given $\tau$}
    \end{align*}

    Let $N_{t}$ denote the no of arrivals till time t, then
    \begin{align*}
        E[N_{t}] &= \lambda t\\
        Var(N_{t}) &= \lambda t
    \end{align*}

    \subsection{Time till kth arrival}
    Suppose the $k^{th}$ arrival happens at a time $t$. Then we are saying that there have been $k-1$ arrivals till time $t$ and the $k^{th}$ arrival happens at time $t$ (precisely in an interval of $[t, t+\delta]$). Let $Y_{k}$ be the required time,
    \begin{align*}
        f_{Y_{k}}(t)\delta &= P(t \leq Y_{k} \leq t+\delta)\\
                    &= P(\text{$k-1$ arrivals  in $[0,t]$}) (\lambda \delta)\\
                    &= \frac{(\lambda t)^{k-1}}{(k-1)!}e^{-\lambda t}(\lambda \delta)\\
        f_{Y_{k}}(t) &= \frac{\lambda^{k} t^{k-1}}{(k-1)!}e^{-\lambda t} \tag*{Erland Distribution}
    \end{align*}

    \subsection{Time of 1st Arrival}
    Using the Erlang Distribution described above, we have
    \begin{align*}
        f_{Y_{1}}(t) = \lambda e^{-\lambda t}
    \end{align*}
    $Y_{k} = T_{1} + T_{2} + \cdots + T_{k}$ where all $T_{i}$ are independent and exponential distributions.

    \subsection{Merging of Poisson Processes}
    Merging of two Poisson processes is also a Poisson process. Consider two flasbulbs of Red and Green colours, flashing as Possion processes with rates $\lambda_{1}$ and $\lambda_{2}$. Then the process denoting the combined flashing of the two bulbs is also Poisson.\newline
    Consider a very small interval of time $\delta$. In this small interval, any of the individual bulbs can have at most one flashes (since we ignore higher order terms). Thus, the following four possibilities arise \newline
    \begin{table}[h]
    \centering
    \begin{tabular}{c|c|c}
        0 & $Red$ & $\overline{Red}$\\ \hline
        $Green$ & $\lambda_{1} \delta \lambda_{2} \delta $ & $(1-\lambda_{1}\delta)  \lambda_{2} \delta$\\ \hline
        $\overline{Green}$ & $\lambda_{1} \delta (1-\lambda_{2}\delta) $ & $(1-\lambda_{1}\delta) (1-\lambda_{2}\delta)$ \\
    \end{tabular}
    \caption{Base Probabilities for flashes}
    \end{table}
    \begin{table}[h]
    \centering
    \begin{tabular}{c|c|c}
        0 & $Red$ & $\overline{Red}$\\ \hline
        $Green$ & $0$ & $ \lambda_{2} \delta$\\ \hline
        $\overline{Green}$ & $\lambda_{1} \delta$ & $(1-(\lambda_{1} + \lambda_{2}) \delta)$ \\
    \end{tabular}
    \caption{Probabilities after ignoring $\delta^{2}$ terms}
    \end{table}

    Thus, the combined process is Poisson with parameter $\lambda_{1} + \lambda_{2}$ \newline
    \begin{align*}
    P(\text{arrival happened from first process}) = \frac{\lambda_{1} \delta}{\lambda_{1} \delta + \lambda_2 \delta} = \frac{\lambda_{1}}{\lambda_{1} + \lambda_{2}}
    \end{align*}

    \subsection{Splitting of Poisson Process}
    Suppose we have a Poisson process with parameter $\lambda$ which we split into two processes up and down, with probabilities $p$ and $1-p$. The two resulting processes are also Poisson with different parameters.\newline
    Consider a small time slot of length $\delta$. Then,
    \begin{align*}
        P(\text{arrival in this time slot}) &= \lambda \delta\\
        P(\text{arrival in up slot}) &= \lambda \delta p\\
        P(\text{arrival in down slot}) &= \lambda \delta (1-p)
    \end{align*}
    Thus, up and down are themselves Poisson with parameters $\lambda p$ and $\lambda (1-p)$ respectively.

    \subsection{Random Indcidence for Poisson}
    Suppose we have a Poisson process with parameter $\lambda$ running forever. We show up at a random time instant. What is the length of the chosen interarrival time (the total of the time from the last arrival to the next arrival).\newline
    Let $T_{1}^{'}$ denote the time that has elapsed since the last arrival and $T_{1}$ be the time till the next arrival. Note that the reverse process is also Poisson with the same parameter. Thus,
    \begin{align*}
        E[\text{interarrival time}] = E[T_{1}^{'} + T_{1}] = \frac{1}{\lambda} + \frac{1}{\lambda} = \frac{2}{\lambda}
    \end{align*}

    This may seem paradoxical since the time difference between any two arrivals in a Poisson process is same and it's expected length is $\frac{1}{\lambda}$, whereas we got an interval twice this length. The paradox is resolved by considering the fact that when we choose a random point in time, it is more likely to fall in an interval of larger size than the smaller ones (since probability will be proportional to the length of the interval).\newline

    Consider a separate example where we want to compare two values $E[\text{size of a family}]$ and $E[\text{size of a family of a given person}]$.\newline
    The two value will be different due to the underlying nature of the way experiment is conducted. For the first, we randomly choose families and average their sizes. Here, family of any size is equally likely to be picked. In the second case, we first pick a person from the population, get their family size, and then average the sizes of the families. Note that, this experiment is biased since the we are more likely to select people from larger families (or equivalently, it is more likely that we pick a person from a large family since the probability of picking is proportional to the family size). Hence, the second value will likely be larger and the two quantities are not equal.

\end{document}