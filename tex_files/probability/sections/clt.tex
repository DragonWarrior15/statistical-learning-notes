\documentclass[../probability-notes.tex]{subfiles}
\begin{document}
    \chapter{Central Limit Theorem}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Weak Law of Large Numbers}
    Suppose we want to know the mean height of penguins in the world. The absolutely correct answer can be obtained by taking the average of the entire population. But ths is not practical, and often we will have to resort to estimating the quantity through a sample. Let there be $n$ penguins in the sample and $X_{1}, X_{2}, \ldots, X_{n}$ be the random variables denoting their heights. Then,
    \begin{align*}
        M_{n} &= \frac{X_{1} + X_{2} + \cdots + X_{n}}{n}\\
        \lim_{n \to \inf} E[M_{n}] &= E[X] = \text{The true mean}
    \end{align*}

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Markov Inequality/Chebychev Inequality}
    For any non-negative random variable $X$ and positive $a$,
    \begin{align*}
        P(X \geq a) \leq \frac{E[X]}{a}
    \end{align*}

    This can be proved as follows
    \begin{align*}
        E[X] &= \int_{0}^{\inf} xp_{X}(x) dx = \int_{0}^{a} xp_{X}(x) dx + \int_{a}^{\inf} xp_{X}(x) dx\\
        &\geq \int_{a}^{\inf} xp_{X}(x) dx \geq \int_{a}^{\inf} ap_{X}(x) dx = a\int_{a}^{\inf} p_{X}(x) dx\\
        &\geq aP(X \geq a)
    \end{align*}

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Chebychev Inequality}
    For any non-negative random variable $X$ with mean $\mu$ and variance $\sigma^{2}$, and a positive $k$,
    \begin{align*}
        P(\lvert X - \mu \rvert \geq k) \leq \frac{\sigma^{2}}{k^{2}}
    \end{align*}
    This can be proved using Markov's inequality on $(X-\mu)^{2}$ and $k^{2}$
    \begin{align*}
        P((X-\mu)^{2} \geq k^{2}) \leq \frac{E[(X-\mu)^{2}]}{k^{2}}\\
        \text{or, } \; P(\lvert X - \mu \rvert \geq k) \leq \frac{\sigma^{2}}{k^{2}}
    \end{align*}

    Substituiting $k = c\sigma$,
    \begin{align*}
        P(\lvert X - \mu \rvert \geq c\sigma) \leq \frac{1}{c^{2}}
    \end{align*}

    Going back to the problem of estimating the mean,
    \begin{align*}
        M_{n} &= \frac{X_{1} + X_{2} + \cdots + X_{n}}{n} \\
        E[M_{n}] &= \frac{1}{n} \sum_{i=1}^{n} E[X_{i}] = \mu \text{\;\; expectation of expectation} \\
        Var(M_{n}) &= \sum_{i=1}^{n} Var(\frac{X_{i}}{n}) = \frac{\sigma^{2}}{n} \text{\;\; since $X_{i}$ are independent} \\
        \Aboxed{P(\mid M_{n} - \mu \mid \geq \epsilon) &\leq \frac{\sigma^{2}}{n\epsilon^{2}}}
    \end{align*}
    or, as $n \rightarrow \inf,\; M_{n} - \mu \rightarrow 0$, $\epsilon$ is the error bound/confidence.

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Central Limit Theorem}
    Chebychev's inequality gives a loose bound. We can do better with CLT. Let $X$ be a random variable with mean $\mu$ and variance $\sigma^{2}$, and let $X_{i}$ be independent identically distributed random variables with the same distribution as $X$. Then,
    \begin{align*}
        S_{n} &= X_{1} + X_{2} + \cdots + X_{n}\\
        Z_{n} &= \frac{S_{n} - E[S_{n}]}{\sigma_{n}} \text{\;\;random variable with mean $0$ and variance $1$} \\
             &= \frac{S_{n} - nE[X]}{\sqrt{n} \sigma}\\
        \text{or,\;\;} S_{n} &= \sqrt{n} \sigma Z_{n} + nE[X] \\
        \text{In\;\;} \lim_{n \to \inf} Z_{n} &\rightarrow Z \text{\;(standard normal)}\\
        \text{or,\;\;} \Aboxed{Z &= \frac{S_{n} - nE[X]}{\sqrt{n} \sigma}} \text{\;\;only for CDF (no comment on PDF/PMF)}\\
        \text{Thus,\;\;} \Aboxed{P(Z > c) &= P(\frac{S_{n} - nE[X]}{\sqrt{n} \sigma} > c)}
    \end{align*}
    By defining the confidence on how close we desire $S_{n}$ to the actual mean, we can calculate the required value of the $n$ using standard normal CDF tables. However, we need to have an estimate of variance of the distribution in order to do the estimate of $n$.
\end{document}
