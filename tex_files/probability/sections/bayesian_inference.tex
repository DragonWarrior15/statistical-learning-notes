\documentclass[../probability-notes.tex]{subfiles}
\begin{document}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Bayesian Inference}
    We have a signal $S$ that goes through a "model" $a$ through which we observe $X$ (with sum added noise $N$). The aim of Bayesian Inference is to try to infer $S$ given the observed $X$.
    \newline
    Hypothesis testing is done on an unknown that takes some possible values, and the aim is to arrive at a value that gives a small probability of incorrect decision (e.g. - Radar)
    \newline
    Estimation is aimed at finding the value of a quantity with a small estimation error (e.g. poll estimation)
    \newline
    Bayes Rule
    \begin{align*}
        p_{\Theta|X}(\theta|x) &= \frac{p_{\Theta}(\theta)p_{X|\Theta}(x|\theta)}{p_{X}(x)} \tag*{$\theta$ and $X$ are both discrete}\\
        \text{or,\;\;} Posterior &= \frac{Prior * Model}{Data}\\
        p_{\Theta|X}(\theta|x) &= \frac{p_{\Theta}(\theta)f_{X|\Theta}(x|\theta)}{f_{X}(x)} \tag*{$\theta$ is discrete and $X$ is continuous}\\
    \end{align*}
    Note that Bayesian inference will give us a distribution over the possible values, but it is often desirable to get an estimate.

    \subsection{Maximum a Posteriori (MAP)}
    MAP is a point estimate of the unknown quantity and is defined as follows
    \begin{align*}
        p_{\Theta|X}(\theta|x) = \max_{\theta}p_{\Theta|X}(\theta|x) \tag*{$\theta$ with maximum posterior probability}\\
    \end{align*}
    In continuous case, expected value can be a better estimate

    \subsection{Maximum Likelihood Estimation}
    This is another method to give estimates from the Bayesian Inference. We assume the random variable of interest to be generated through a model with some parameters, i.e. $X \sim p_{X}(x;\theta)$ and we pick the $\theta$ that makes the data most likely
    \begin{align*}
        \hat{\theta}_{MLE} &= \argmax_{\theta} p_{X|\Theta}(x|\theta)\\
        \hat{\theta}_{MAP} &= \argmax_{\theta} p_{\Theta|X}(\theta|x)\\
        &= \argmax_{\theta} p_{X|\Theta}(x|\theta)p_{\Theta}(\theta)
    \end{align*}

    Thus, we can see that if we assume a uniform prior on $\theta$, MAP and MLE estimates are the same. MLE estimates tha maximum through cosideration of multiple probabilistic models. We can get different estimates for different priors.


    \subsection{Least Mean Square Estimate}
    Here, we aim to find an estimate such that
    \begin{align*}
        \theta* &= \min_{c} E[(\Theta - c)^{2}]\\
        E[(\Theta - c)^{2}] &= E[\Theta^{2}] - 2cE[\Theta] + c^{2}\\
        \text{Taking derivative,\;\;} \frac{dE}{dc} &= 0\\
        \Aboxed{c &= E[\Theta]}\\
        \text{In general,\;\;} c &= E[\Theta|X] \tag*{minimizes $E[(\Theta - g(X))^{2}]$ over all estimators $g(X)$ }
    \end{align*}
    $E[\Theta]$ minimizes the least squares estimate
    \newline
    When $X$ is observed, the best estimate simlply becomes $E[\Theta|X]$.

\end{document}