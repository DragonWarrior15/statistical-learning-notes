\documentclass[../probability-notes.tex]{subfiles}
\begin{document}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Markov Process}
    Markov Process is a discrete time process that is not memoryless. Here the random variable takes several possible states, and the probability distribution is defined in such a way that $P(\text{transition from state 1 to state 2})$ is dependent on state 1.\newline

    Let $X_{n}$ be the random variable denoting the state after n transitions and $X_{0}$ will represent the starting state (which can be given or random). Markov assumption states that \emph{Given the current state, past does not matter}. Armed with these,
    \begin{align*}
        p_{ij} &= P(\text{next state $j$ $|$ current state $i$})\\
        p_{ij} &= P(X_{n+1}=j|X_{n}=i) = P(X_{n+1}=j|X_{n}=i, X_{n-1}, \ldots, X_{0})\\
        r_{ij}(n) &= P(X_{n}=j|X_{0}=i) \tag*{or, in state $j$ after $n$ steps}\\
        r_{ij}(n) &= \sum_{k=1}^{m} r_{ik}(n-1)p_{kj}
    \end{align*}

    \subsection{Recurring and Transient States}
    A state $i$ is called \emph{recurrent} if, starting from $i$, and travelling anywhere, it is always possible to return to $i$. If a state is not recurrent, it is \emph{transient}. States in a recurrent class are periodic if they can be grouped into $d > 1$ groups so that all transitions from one group lead to the next group.

    \subsection{Steady State Probabilities}
    \label{sec_markov_steady}
    Do $r_{ij}(n)$ converge to some $\pi_{j}$ (independent of i) ? \newline
    Yes if,
    \begin{itemize}
        \item recurrent states are all in a single class
        \item single recurrent class is not periodic (otherwise oscillations are possible)
    \end{itemize}
    Assuming yes,
    \begin{align*}
        r_{ij}(n) &= \sum_{k} r_{ik}(n-1)p_{kj}\\
        \lim_{n \to \inf} r_{ij}(n) &= \sum_{k} r_{ik}(n-1)p_{kj}\\
        \pi_{ij} &= \sum_{k} \pi_{ik} p_{kj} \tag*{balance equations} \\
        \sum_{i} \pi_{i} &= 1 \\
        \text{frequency of transitions $k \rightarrow j$} &= \pi_{k} p_{kj} \tag*{in one step}\\
        \text{frequency of transitions into $j$} &= \sum_{k} \pi_{k} p_{kj} \tag*{influx from all connected states}
    \end{align*}

    \subsection{Birth Death Process}
    Consider the checkout counter example. The states are represented by the number of people currently being processed, and we always move $n$ to $[n-1, n, n+1]$, i.e., either the people in the queue decrease by one, remain same or increase by one. Let the probability for moving up be $p$ and moving down be $q$.

    \begin{center}
    \begin{tikzpicture}
        % first add the node that we want to represent
        \node[state]             (0) {$0$};
        \node[state, right=of 0] (1) {$1$};
        \node[state, right=of 1] (2) {$2$};
        \node[draw=none, right=of 2] (3) {$\cdots$};
        \node[state, right=of 3] (4) {$m-1$};
        \node[state, right=of 4] (5) {$m$};
        % draw the edges between the nodes
        % bend left/right is from the persepective of the starting node
        % and so is the auto=left/right which specifies the side to put text
        \draw[every loop]
            % right edges
            (0) edge[bend left, auto=left] node {$p_{0}$} (1)
            (1) edge[bend left, auto=left] node {$p_{1}$} (2)
            (2) edge[bend left, auto=left] node {$p_{2}$} (3)
            (3) edge[bend left, auto=left] node {$p_{m-2}$} (4)
            (4) edge[bend left, auto=left] node {$p_{m-1}$} (5)
            % left edges
            (1) edge[bend left, auto=left] node {$q_{1}$} (0)
            (2) edge[bend left, auto=left] node {$q_{2}$} (1)
            (3) edge[bend left, auto=left] node {$q_{3}$} (2)
            (4) edge[bend left, auto=left] node {$q_{m-1}$} (3)
            (5) edge[bend left, auto=left] node {$q_{m}$} (4)
            % self loops
            (0) edge[loop above] node {$1-p_{0}$} (0)
            (1) edge[loop above] node {$1-p_{1}-q_{1}$} (1)
            (2) edge[loop above] node {$1-p_{2}-q_{2}$} (2)
            (4) edge[loop above] node {$1-p_{m-1}-q_{m-1}$} (4)
            (5) edge[loop above] node {$1-q_{m}$} (5);
    \end{tikzpicture}
    \end{center}

    Let's estimate the steady state probabilities. Consider the following diagram splitting the chain into two parts through the two adjacent states
    \begin{center}
    \begin{tikzpicture}
        % first add the node that we want to represent
        \node[state]             (0) {$i$};
        \node[state, right=of 0] (1) {$i+1$};
        % draw the edges between the nodes
        % bend left/right is from the persepective of the starting node
        % and so is the auto=left/right which specifies the side to put text
        \draw[every loop]
            % right edges
            (0) edge[bend left, auto=left] node {$p_{i}$} (1)
            % left edges
            (1) edge[bend left, auto=left] node {$q_{i+1}$} (0);
    \end{tikzpicture}
    \end{center}

    In this case, to maintain steady state, long term frequency of left-right transition should be same as right left transition, i.e., $\pi_{i}p_{i} = \pi_{i+1}q_{i}$ \newline
    In the special case of $p_{i} = p$ and $q_{i} = q \;\forall\; i$,
    \begin{align*}
        \rho &= \frac{p}{q} \tag*{load factor}\\
        \pi_{i+1} &= \pi_{i} \frac{p}{q} = \pi_{i} \rho \\
        \pi_{i} &= \pi_{0} \rho^{i} \tag*{$i = 0,\ldots,m$} \\
        \text{Using } \sum_{i=0}^{m} \pi_{0}\rho^{i} &= 1,\\
        \pi_{0} &= \frac{1}{\sum_{i=0}^{m} \rho^{i}}\\
        \text{if $p < q$ and $m \rightarrow \inf,$}\\
        \pi_{0} &= 1 - \rho \\
        \pi_{i} &= (1-\rho)\rho^{i}\\
        E[X_{n}] &= \frac{\rho}{1-\rho} \tag*{Exponential Distribution}
    \end{align*}
    When $\rho = 1$ or $p = q$, then all states are equally likely - symmetric random walk.

    \subsection{Absorption Probabilities}
    \label{sec_markov_absorb}
    let $a_{i}$ denote the probability of absorption and $\mu_{i}$ denote the expected no of steps until absorption starting from state $i$. Then,
    \begin{align*}
        a_{i} &= \sum_{j} a_{j}p_{ij} \tag*{outflux to the possible states}\\
        \mu_{i} &= 1 + \sum_{j} \mu_{j} p_{ij}
    \end{align*}
    For multipe absorption states, we can possibly consider them together as a group and calculate the relevant quantities. \newline
    For a given state $s$,
    \begin{align*}
        E[\text{steps to first time reach $s$ from $i$}] &= t_{i} \\
        t_{i} &= E[min \{n \geq 0 \text{ such that } X_{n} = s\}] \\
        t_{s} &= 0 \\
        t_{i} &= 1 + \sum_{j} t_{j}p_{ij} \tag*{outflux to all possible states}
    \end{align*}

    Mean recurrence time (mean time to reach back a state) for $s$
    \begin{align*}
        t_{s}^{*} &= E[min\{n \geq 1 \text{ such that } X_{n}=s\} | X_{0} = s] \\
        t_{s}^{*} &= 1 + \sum_{j} t_{j} p_{ij}
    \end{align*}
\end{document}