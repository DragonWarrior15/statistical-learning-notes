\documentclass[../statistical_learning_notes.tex]{subfiles}
\begin{document}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \chapter{Tree Based Models}
    The idea behind this family of methods is to segment the region into subspaces and the prediction for any subspace is made by taking the \emph{mean} or \emph{mode} of the response in that region.\newline
    A simple decision tree may not give performance at par with linear regression or generalized additive models, but when combined with techniques like bagging and boosting, multiple trees can yield significant reduction in bias.\newline
    A simple decision tree finds its utility in being relatively simpler in interpretation than it's derived non-linear family.

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Regression Trees}
    The algorithm behind building trees consists of two basic steps
    \begin{enumerate}
        \item Divide the predictors $X_{1}, X_{2}, \ldots, X_{n}$ into $J$ \textbf{distinct} and \textbf{non-overlapping} regions $R_{1}, R_{2}, \ldots, R_{J}$.
        \item For making a prediction in the region $R_{j}$, simply take the \textbf{response mean} of all the data points following in that region.
    \end{enumerate}
    With the above definition, the loss becomes
    \begin{align*}
        RSS = \sum_{j=1}^{J} \sum_{i \in R_{J}} (y_{i} - \bar{y}_{R_{J}})^{2}
    \end{align*}
    But minimizing this loss is infeasible in practice as the total number of possible partitions are too large !\newline

    To overcome this, we build the trees in a greedy top down approach called \textbf{recursive binary splitting}. This method aims to find the best possible split at each level in a greedy manner.\newline

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsection{The Algorithm}
    To start off, the alogithm will try to split the data into two regions $\{X|X_{j} < s\}$ and $\{X|X_{j} \geq s\}$ using the values of predictor $X_{j}$ and the search is performed across all the predictors to choose the one which minimizes the RSS. Mathematically
    \begin{align*}
        R_{1}(j,s) = \{X|X_{j} < s\} \text{ and } R_{2}(j,s) = \{X|X_{j} \geq s\}\\
        \minimize_{j, s} \sum_{i:x_{i} \in R_{1}} (y_{i} - \bar{y}_{R_{1}})^{2} + \sum_{i:x_{i} \in R_{2}} (y_{i} - \bar{y}_{R_{2}})^{2}
    \end{align*}
    where $\bar{y}_{R_{1}}$ and $\bar{y}_{R_{2}}$ are the response mean in the region $R_{1}$ and $R_{2}$ respectively.\newline

    The same algorithm is run recursively. The only change made after the first split is that we now work independently on two separate regions. For each region, we will not be able to consider all the possible values of $s$ since the region has been restricted. Otherwise, the RSS term will remain the same.\newline

    The process is stopped when a pre-determined stopping criteria is reached, such as minimum node size or depth (typically the tree is grown to a large depth and then pruned). For making a prediction, we will first determine the region in which the test observation falls and then make the prediction based on the mean of the training samples in this region (which has already been calculated while building the tree).\newline

    At the first split, $O(pNlog(N))$ cost is needed to sort all the predictors. Then at each split, another $O(pNlog(N))$ cost must be spent to find the best predictor among all the predictors. Further, this cost will be much higher in case we use multiple splits at each level instead of binary splits. The multiple splits can anyways be captured in multiple levels of a binary tree. Hence we stick with binary trees due to easy and efficieny of implementation and computations. 


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsection{Tree Pruning}
    The tree build using above strategy can have good results on the training set, but will yield poor performance on the test set due to the high complexity. A better strategy is to build \textbf{short trees which are less complex and produce lower variance at cost of little bias} and are more interpretable.\newline

    A simple strategy can be to build only when there is a decrease in RSS above a threshold. This approach can be short-sighted as some future good branches can be missed.\newline

    Tree pruning is to \textbf{build a large tree and then shrink it back to obtain a subtree}. The RSS of a subtree can be found via cross validation. This approach is expensive for all possible subtrees.\newline

    \textbf{Cost complexity Pruning} or \textbf{weakest link pruning} is an approach to overcome this problem. For a non-negative parameter $\alpha$, there exists a subtree $T$ of the large tree $T_{0}$ such that the following is minimized
    \begin{align*}
        RSS = \sum_{m=1}^{\vert T \vert} \sum_{i:x_{i}\in R_{m}} (y_{i} - \bar{y}_{R_{m}})^{2} + \alpha \vert T \vert
    \end{align*}
    where $\vert T \vert$ is the number of terminal nodes in the tree. The formulation is similar to lasso and here we are controlling the complexity of the tree via a parameter $\alpha$. Larger the $\alpha$, the less number of terminal nodes there would be in the tree.\newline

    The same formula is valid for any of the internal nodes/subtrees also. Hence, for a given $\alpha$, we start pruning the tree starting with the terminal node that has the lowest reduction in the error metric (also called weakest link pruning). For a given $\alpha$ we obtain a sequence of subtrees and select the one with the minimum overall cost.\newline

    The algorithm can then be summarized as follows
    \begin{enumerate}
        \item Use \emph{binary recursive splitting} to obtain a large tree on the data set. Stop only when at a terminal node, the number of observations is less than a minimum.
        \item Apply \emph{cost complexity pruning} in order to obtain a sequence small trees $T$ and choose the best $T_{\alpha}$ with the minimum cost associated with the parameter $\alpha$.
        \item Use K-fold cross validation to choose $\alpha$. For each of the folds $k = 1, 2, \ldots, K$
        \begin{enumerate}
             \item Repeat steps 1 and 2 excluding the data from the $k$th fold to obtain a family of subtrees as a function of $\alpha$.
             \item Evaluate the mean predicted validation error as a function of $\alpha$.\newline
             Choose the value of $\alpha$ that minimizes the average error across all the folds.
         \end{enumerate}
        \item Select the substree $T_{\alpha}$ from step 2 (trained on the whole dataset) that corresponds to the chosen value of $\alpha$.
    \end{enumerate}
    

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Classification Trees}
    Here we predict a qualitative variable and will report the \textbf{most occurring class in the region}. Since we are almost always also interested in the probabilistic aspects of the prediction, we also consider \textbf{the proportion of the classes occurring in a region}.
    

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsection{The Algorithm}
    The algorithm is almost similar to the regression trees but instead of RSS, we need to use something that is related to class proportions and frequencies. Different types of metrics are available

    \begin{itemize}
        \item \textbf{classification error rate} $= 1 - \max_{k}\hat{p}_{mk}$ where $\hat{p}_{mk}$ is the proportion of the $k$th class in the $m$th region. However, in practice, this metric is not sufficient to grow the trees.
        \item \textbf{Gini Index}, $G = \sum_{k=1}^{K} \hat{p}_{mk} (1 - \hat{p}_{mk}) = 1 - \sum_{k=1}^{K} p_{k}^{2}$ for a given region. in the case of binary classification, this formula becomes $G = 2p(1-p)$ where $p = p(Y=1|X)$. Clearly, G is close to zero when the node is pure, i.e. most of the observations come from the same class. Gini index is also know as a measure of \textbf{purity} since it's low value implies predominance of a single class in the node.
        \item \textbf{Entropy} $D = \sum_{k=1}^{K} -\hat{p}_{mk} \log(\hat{p}_{mk})$ for a region. Entropy takes value close to zero if the class probabilities are close to $0$ or $1$. Gini index and Entropy are somewhat similar numerically.
    \end{itemize}
    In addition to these impurity measures, we also need to weight the two left and right nodes based on the number of observations $N_{L}$ and $N_{R}$, since purifying a large node is preferable than ultra purifying a small node.\newline

    Gini Index and entropy are preferable when going a tree because they are differentiable and thus friendly to numerical optimization. Also, entropy is slightly computationally expensive to compute due to the logarithm. Hence, most packages will prefer gini index over entropy.\newline
    For pruning a tree, all three work and misclassification rate is frequently used.

    %%%%%%%%%%%%%%%%%%%%
    \subsubsection*{Categorical Predictors}
    The classes of a categorical predictor can be ordered based on the mean of the response (in case of classification trees, the mean of response is simply the fraction of 1s in that class). To use in a tree, we simply use this transformed version similar to continuous variable. The best split point is chosen and all classes with higher response mean fall on one side and the remaining on the other. This simple trick helps avoid checking all possible spilts with differing number of classes on each side (Proof can be found in Breiman et al. (1984) and Ripley (1996)). Larger number of classes help find better split point but also have a higher risk of overfit since examples in a particular class might become less.


    %%%%%%%%%%%%%%%%%%%%
    \subsubsection*{High Variance}
    High variance is inherent to trees due to the nature of their construction. Slight changes in the data can cause an entirely different tree to be built. Errors at any level are propagated to all the levels below. This can cause instability in making interpretation and prediction. Bagging is an approach that helps alleviate the problem of variance at a slight cost of interpretability.


    %%%%%%%%%%%%%%%%%%%%
    \section{Patient Rule Induction Method (PRIM)}
    The CART method tries to partition the whole input space into boxes and the aim is to make those boxes as different as possible. PRIM on the other hand, tries to find boxes which have higher response mean (or trying to find a \emph{bump} in the input space). This is achieved as follows

    \begin{enumerate}
        \item A big bounding box is created containing all the data in the training set.
        \item A small fraction $\alpha$ (0.05 or 0.1) of observations are removed from one face (the higher or lower end of one of the predictors) such that the response of the mean is higher in the remaining box (\emph{peeling}).
        \item The process is repeated under the box contains a minium number of observations.
        \item Since the approach was greedy, it is possible that some optimal box was left out. Hence, we expand the box by expanding one face at a time as long as the box response mean is increasing (\emph{pasting}).
        \item Among this sequence of boxes, the best one is chosen via cross validation.
        \item All the data in this chosen box is removed and the process is continued with the reamining observations.
    \end{enumerate}

    This approach works well with regression and binary classification (0-1 encoding of classes). The advantage of PRIM over CART is it's patience and slow removal of data. CART splits data too quickly using at most $log_{2}(N)$ splits, while PRIM will use roughly $-log(N)/log(1-\alpha)$ steps (this can be seen by considering the approximate steps as $N(1-\alpha)^{steps} = 1$ and solving by taking the log). The rules for any box are a set of double sided inequalities, and may make the approach less interpretable than CART. 


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Bagging}
    Decision tree suffer from a high variance. When the data is divided into smaller parts, it is quite likely to expect variance when the data set itself is changed. Bootstrap Aggregation of Bagging is a method to try to overcome this problem.\newline

    \textbf{Averaging a set of independent random variables reduces variance}. Consider $n$ random variables with the same variance $\sigma^{2}$, then the variance of their average is $\sigma^{2}/n$. Naturally, we can extend this idea to build several prediction models on different training data sets and average out their results to reduce the variance in the response.\newline

    We train $b$ different models, make predictions $\hat{f}_{i}(x)$ using these and then average them out to get a low variance
    \begin{align*}
        \hat{f} = \frac{1}{B}\sum_{b=1}^{B} \hat{f}_{b}(x)
    \end{align*}
    
    This reduction is obvious when the trees are completely independent. This is not true in majority of the cases. Since the trees are deep, we expect them to have very low bias, and similar expectations/means. The noise is introduced due to their variance. Suppose the trees come from the same distribution, but have some pairwise correlation $\rho$
    \begin{gather*}
        E[\hat{f}_{b}(x)] = \mu, \quad Var(\hat{f}_{b}(x)) = \sigma^{2} = E[\hat{f}_{b}(x)^{2}] - \mu^{2}\\
        \rho = \frac{E[(\hat{f}_{i}(x) - \mu)(\hat{f}_{j}(x) - \mu)]}{\sqrt{Var(\hat{f}_{i}(x))Var(\hat{f}_{j}(x))}} = \frac{E[\hat{f}_{i}(x)\hat{f}_{j}(x) - \mu^{2}]}{\sigma^{2}}
    \end{gather*}
    Then, variance of the average of trees
    \begin{align*}
        Var(\frac{1}{B} \sum_{b=1}^{B} \hat{f}_{b}(x)) &= E[\big(\frac{1}{B} \sum_{b=1}^{B} \hat{f}_{b}(x)\big)^{2}] - E[\big(\frac{1}{B} \sum_{b=1}^{B} \hat{f}_{b}(x)\big)]^{2}\\
        &= \frac{1}{B^{2}} \big(E[\sum_{b=1}^{B} \hat{f}_{b}(x)^{2}] + \sum_{i<j}E[\hat{f}_{i}(x)\hat{f}_{j}(x)] \big) - \mu^{2}\\
        &= \frac{1}{B^{2}}\big( B\mu^{2} + B(B-1)(\sigma^{2} + \mu^{2}) \big) - \mu^{2}\\
        &= \rho \sigma^{2} + \frac{1-\rho}{B} \sigma^{2} \numberthiseqn\label{eq:var_avg_trees}
    \end{align*}
    When the number of trees is large enough, $\rho$ will decide how much the variance shrinks.\newline

    Since we do not have access to different training datasets, we will get the average using bootstraps of the original data set. Note that \textbf{trees grown on bootstrapped data sets are deep and not pruned} so that they have low bias. Even though they may have high variance, averaging will reduce it out.\newline

    \textbf{Averaging bootstrapped predictors works for regression} while we \textbf{take the majority vote in case of classification}. The overall prediction is the most commonly occurring class across the $B$ trees. Note that \textbf{using a large value of \emph{B} will not lead to overfitting}, it should just be sufficiently large to ensure the error has come down compared to a single tree (building too many trees can take up significant time).
    

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsection{Out of Bag Error}\label{sec:oob_error}
    It can be shown the probability of an observation being present in a bootstrapped dataset is $1 - 1/e$ (this can be shown by considering that the probability that an observation is not present in the data set is $((1 - 1/n)^{n}$ and taking the limit $n \to \inf$). Thus, on an average, a data will only contain about $2/3$rd ($0.63$ to be exact) of the total observations in it. We can make use of the observations not used for training to estimate the error.\newline

    Any observation not part of the bootstrapped set is called \textbf{Out of Bag} and the error we are about to calculate is out of bag error. Now, we can take the $i$th observation, and get it's prediction from the sets where it was not used for training. This will be around $1/3$rd of the predictors. We can combine these results using average in case of regression or majority vote in case of classification. Thus, we have a "test error" for all the observations in the data.

    
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsection{Variable Importance}
    We have been able to reduce the variance using the bagging approach, but the model is no longer interpretable due to the presence of multiple trees. We need to somehow aggregate all the trees to get this measure.\newline

    A simple workaround is to calculate the total amount the RSS (regression) or Gini Index (classification) has decreased across all the trees due to split on a particular variable. To make the values comparable, we simply take the average across $B$ trees. A large value means that the variable has high importance.
    

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subfile{random_forest}

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Boosting}
    Boosting is a general tecnique that can be applied to multiple statistical learning techniques and not just decision trees. Like Bagging, boosting is also trained on multiple data sets derived from the training data, but instead of building independent trees on different data sets, \textbf{boosting is a sequential process that builds trees on modified versions of the original data set}.\newline

    Given a model, boosting will try to fit a tree on the residuals obtained by the model instead of the response $Y$. The model is then updated by adding this tree and the residuals are calculated again. \textbf{Trees in boosting are typically small, just a few terminal nodes} controlled by the parameter $d$ and we allow the tree to fit the areas where improvement is needed in a slow manner. Shrinkage parameter $\lambda$ allows to further slow down this process allowing for a variety of trees.\newline

    Boosting has three parameters
    \begin{enumerate}
         \item $B$, the number of trees. Unlike bagging, large $B$ can overfit although slowly. Right value of $B$ can be determined through cross validation.
         \item Shrinkage parameter $\lambda$ which controls how slowly the model is learnt and is a small positive number in the range 0.1 to 0.001 typically. Very smal $\lambda$ can require a very large $B$ to achieve a good performance.
         \item The number of splits $d$ in a single tree. Typically $d=1$ is used and referred to a stump. Higer value signifies higher interaction between the variables. When using stumps, the model is simply additive since we are using a single variable in each of the trees.
     \end{enumerate}

     Boosting algorithm is as follows
     \begin{enumerate}
         \item Set the residuals $r_{i} = y_{i}$ and the model $\hat{f}(x) = 0$.
         \item For $b = 1, \cdots, B$ repeat
         \begin{enumerate}
             \item Fit the model $\hat{f}^{b}(x)$ with $d$ splits on the data $(X,r)$
             \item Update the model
             \begin{align*}
                 \hat{f}(x) \leftarrow \hat{f}(x) + \lambda \hat{f}^{b}(x)
             \end{align*}
             \item Update the residuals
             \begin{align*}
                 r_{i} \leftarrow r_{i} - \lambda \hat{f}^{b}(x)
             \end{align*}
         \end{enumerate}
         \item Return the boosted model
         \begin{align*}
             \hat{f}(x) = \sum_{b=1}^{B} \lambda \hat{f}^{b}(x)
         \end{align*}
     \end{enumerate}

    
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subfile{adaboost}
    
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subfile{boosting_trees}
    
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subfile{xgboost}

\end{document}