\documentclass[11pt, a4paper]{article}

\usepackage{amsmath}
\usepackage[a4paper, margin=1in]{geometry}
\usepackage{amsfonts}
\usepackage{mathtools}

% to insert images
% \usepackage{graphicx}
% \graphicspath{ {./images/} {../images/}}

% \usepackage{hyperref}
% \hypersetup{
    % colorlinks=true, % make the links colored
    % linkcolor=blue, % color TOC links in blue
    % urlcolor=red, % color URLs in red
    % linktoc=all % 'all' will create links for everything in the TOC
% }

\setlength{\parindent}{0em}

\DeclareMathOperator*{\minimize}{minimize}


\begin{document}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Tree Based Models}
    The idea behind this family of methods is to segment the region into subspaces and the prediction for any subspace is made by taking the \emph{mean} or \emph{mode} of the response in that region.\newline
    A simple decision tree may not give performance at par with linear regression or generalized additive models, but when combined with techniques like bagging and boosting, multiple trees can yield significant reduction in bias.\newline
    A simple decision tree finds its utility in being relatively simpler in interpretation than it's derived non-linear family.

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsection{Regression Trees}
    The algorithm behind building trees consists of two basic steps
    \begin{enumerate}
        \item Divide the predictors $X_{1}, X_{2}, \ldots, X_{n}$ into $J$ \textbf{distinct} and \textbf{non-overlapping} regions $R_{1}, R_{2}, \ldots, R_{J}$.
        \item For making a prediction in the region $R_{j}$, simply take the \textbf{response mean} of all the data points following in that region.
    \end{enumerate}
    With the above definition, the loss becomes
    \begin{align*}
        RSS = \sum_{j=1}^{J} \sum_{i \in R_{J}} (y_{i} - \bar{y}_{R_{J}})^{2}
    \end{align*}
    But minimizing this loss is infeasible in practice as the total number of possible partitions are too large !\newline

    To overcome this, we build the trees in a greedy top down approach called \textbf{recursive binary splitting}. This method aims to find the best possible split at each level in a greedy manner.\newline

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsubsection{The Algorithm}
    To start off, the alogithm will try to split the data into two regions $\{X|X_{j} < s\}$ and $\{X|X_{j} \geq s\}$ using the values of predictor $X_{j}$ and the search is performed across all the predictors to choose the one which minimizes the RSS. Mathematically
    \begin{align*}
        R_{1}(j,s) = \{X|X_{j} < s\} \text{ and } R_{2}(j,s) = \{X|X_{j} \geq s\}\\
        \minimize_{j, s} \sum_{i:x_{i} \in R_{1}} (y_{i} - \bar{y}_{R_{1}})^{2} + \sum_{i:x_{i} \in R_{2}} (y_{i} - \bar{y}_{R_{2}})^{2}
    \end{align*}
    where $\bar{y}_{R_{1}}$ and $\bar{y}_{R_{2}}$ are the response mean in the region $R_{1}$ and $R_{2}$ respectively.\newline

    The same algorithm is run recursively. The only change that is made that after the first split, we now wolr independently on two separate regions. For each region, we will not be able to consider all the possible values of $s$ since the region has been restricted. Otherwise, the RSS term will remain the same.\newline

    The process is stopped when a pre-determined stopping criteria is reached. For making a prediction, we will first determine the region in which the test observation falls and then make the prediction based on the mean of the training samples in this region.


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsubsection{Tree Pruning}
    The tree build using above strategy can have good results on the training set, but will yield poor performance on the test set due to the high complexity. A better strategy is to build \textbf{short trees which are less complex and produce lower variance at cost of little bias} and are more interpretable.\newline

    A simple strategy can be to build only when there is a decrease in RSS above a threshold. This approach can be short-sighted as some future good branches can be missed.\newline

    Tree pruning is to \textbf{build a large tree and then shrink it back to obtain a subtree}. The RSS of a subtree can be found via cross validation. This approach is expensive for all possible subtrees.\newline

    \textbf{Cost complexity Pruning} or \textbf{weakest link pruning} is an approach to overcome this problem. For a non-negative parameter $\alpha$, there exists a subtree $T$ of the large tree $T_{0}$ such that the following is minimized
    \begin{align*}
        RSS = \sum_{m=1}^{\vert T \vert} \sum_{i:x_{i}\in R_{m}} (y_{i} - \bar{y}_{R_{m}})^{2} + \alpha \vert T \vert
    \end{align*}
    where $\vert T \vert$ is the number of terminal nodes in the tree. The formulation is similar to lasso and here we are controlling the complexity of the tree via a parameter $\alpha$. Larger the $\alpha$, the less number of terminal nodes there would be in the tree.\newline

    The algorithm can then be summarized as follows
    \begin{enumerate}
        \item Use \emph{binary recursive splitting} to obtain a large tree on the data set. Stop only when at a terminal node, the number of observations is less than a minimum.
        \item Apply \emph{cost complexity pruning} in order to obtain a small tree $T$ as a function of the cost parameter $\alpha$ (fix a set of $\alpha$ and get a set of subtrees).
        \item Use K-fold cross validation to choose $\alpha$. For each of the folds $k = 1, 2, \ldots, K$
        \begin{enumerate}
             \item Repeat steps 1 and 2 excluding the data from the $k$th fold to obtain a family of subtrees as a function of $\alpha$.
             \item Evaluate the mean predicted validation error as a function of $\alpha$.\newline
             Choose the value of $\alpha$ that minimizes the average error across all the folds.
         \end{enumerate}
        \item Select the substree from step 2 that corresponds to the chosen value of $\alpha$.
    \end{enumerate}
    

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsection{Classification Trees}
    Here we predict a qualitative variable and will report the \textbf{most occurring class in the region}. Since we are almost always also interested in the probabilistic aspects of the prediction, we also consider \textbf{the proportion of the classes occurring in a region}.
    

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsubsection{The Algorithm}
    The algorithm is almost similar to the regression trees but instead of RSS, we need to use something that is related to class proportions and frequencies. A very straightforward metric is \textbf{classification error rate}
    \begin{align*}
        E = 1 - \max_{k}\hat{p}_{mk}
    \end{align*}
    where $\hat{p}_{mk}$ is the proportion of the $k$th class in the $m$th region. However, in practice, this metric is not sufficient to grow the trees.

    
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsubsection{Gini Index}
    Gini Index is defined as
    \begin{align*}
        G = \sum_{k=1}^{K} \hat{p}_{mk} (1 - \hat{p}_{mk})
    \end{align*}
    for a given region. Clearly, G is close to zero when the node is pure, i.e. most of the observations come from the same class. Gini index is also know as a measure of \textbf{purity} since it's low value implies predominance of a single class in the node.
    

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsubsection{Entropy}
    Entropy is defined as
    \begin{align*}
        D = \sum_{k=1}^{K} -\hat{p}_{mk} \log(\hat{p}_{mk})
    \end{align*}
    for a region. Entropy takes value close to zero if the class probabilities are close to $0$ or $1$. Gini index and Entropy are somewhat similar numerically.


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsection{Bagging}
    Decision tree suffer from a high variance. When the data is divided into smaller parts, it is quite likely to expect variance when the data set itself is changed. Bootstrap Aggregation of Bagging is a method to try to overcome this problem.\newline

    \textbf{Averaging a set of random variables reduces variance}. Consider $n$ random variables with the same variance $\sigma^{2}$, then the variance of their average is $\sigma^{2}/n$. Naturally, we can extend this idea to build several prediction models on different training data sets and average out their results to reduce the variance in the response.\newline

    We train $b$ different models, make predictions $\hat{f}_{i}(x)$ using these and then average them out as
    \begin{align*}
        \hat{f} = \frac{1}{B}\sum_{b=1}^{B} \hat{f}_{b}(x)
    \end{align*}
    to get a low variance prediction statistic.\newline

    Since we do not have access to different training datasets, we will get the average using bootstraps of the original data set. Note that \textbf{trees grown on bootstrapped data sets are deep and not pruned} so that they have low bias. Even though they may have high variance, averaging will reduce it out.\newline

    \textbf{Averaging bootstrapped predictors works for regression} while we \textbf{take the majority vote in case of classification}. The overall prediction is the most commonly occurring class across the $B$ trees. Note that \textbf{using a large value of \emph{B} will not lead to overfitting}, it should just be sufficiently large to ensure the error has come down compared to a single tree (building too many trees can take up significant time).
\end{document}