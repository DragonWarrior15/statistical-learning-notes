\documentclass[11pt, a4paper]{article}

\usepackage{amsmath}
\usepackage[a4paper, margin=1in]{geometry}
\usepackage{amsfonts}
\usepackage{mathtools}
% \usepackage{algorithm}

% to insert images
\usepackage{graphicx}
\graphicspath{ {./images/} {../images/}}

% \usepackage{tikz}
% \usetikzlibrary{automata, positioning}

% \usepackage{pgfplots}
% \pgfplotsset{width=5cm,compat=1.9}
\setlength{\parindent}{0em}

\DeclareMathOperator*{\minimize}{minimize}
\DeclareMathOperator*{\maximize}{maximize}

\begin{document}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Linear Model Selection and Regularization}
    Linear models are often simple and easy to interpret at the cost of having high bias if the relationship in the data is not linear. Some considerations about linear models
    \begin{itemize}
        \item If $n >> p$, least square estimates often have less variance. If $n$ is larger than $p$, then least square estimates can have some variance. While if $n < p$, we are looking at non unique solutions which can cause lot of variation in the test predictions.
        \item It is often the case that many of the predictors do not have a relationship with the response. Hence, it is a good idea to remove those and make the model more interpretable at the cost of some bias. Least square estimates almost never give zero coefficients.
    \end{itemize}

    There are major ways in which the number of variables in the model can be reduced
    \begin{itemize}
        \item Selecting a \textbf{subset of variables} that go well with the response. This itself can be done by forward selection, backward elimination etc.
        \item \textbf{Shrinking} some of the \textbf{coefficients} to zero. This is a great help in reducing the variance of the predictions.
        \item \textbf{Dimension Reduction} helps in projecting the $p$ predictors onto a $M$ dimensional space where $M < p$. This utilizes linear combinations to create a set of new features.
    \end{itemize}

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsection{Subset Selection}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsubsection{Best Subset Selection}
    This is a naive approach that essentially tries to find the best model among $2^{p}$ models that are trained on all possible subsets of the $p$ variables. As we increase the subset of variables, the training error will monotonically decrease whereas the same cannot be said for the test error. A number of criteria like test MSE, $R^{2}$, AIC etc can be used to pick the models.\newline

    In case of classification models, similar argument holds and a more general error metric $deviance$ can be used. $Deviance$ is defined as $-2 * \log likelihood$ of the data. The smaller the $deviance$, the better the model fit.\newline

    The huge search space presented by this approach easily overfits as the search space presents more opportunities to find better fits. However, this causes a higher variance in the predictions on future data and can possibly also have higher test error.

    
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsubsection{Forward Stepwise Selection}
    This is a greedy approach that significantly shrinks the search space being checked (in comparison to the best subset selection approach).

    Forward Stepwise Selection Algorithm
    \begin{enumerate}
        \item Let $M_{0}$ denote the null model, i.e., the model with no predictors
        \item For $k = 0, 1, \ldots, p - 1$
        \begin{enumerate}
            \item Consider all $p - k$ models formed by adding a single predictor to the model $M_{k}$
            \item Select the best model $M_{k+1}$ among the $p - k$ models on the basis of the error metric
        \end{enumerate}
        \item From the models $M_{0}, M_{1}, \ldots, M_{p}$, select the one with the lowest cross validation error on the evaluation choosing the appropriate error metric
    \end{enumerate}

    This approach effectively has reduced the search space from $2^{p}$ to $1 + p(p+1)/2$. Although, now it is not guaranteed that the model selected will be the best one among $2^{p}$.


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsubsection{Backward Stepwise Selection}
    This approach is the opposite of forward stepwise selection. We recursively reduce the number of variables in our model.

    \begin{enumerate}
        \item Let $M_{p}$ denote the complete model, i.e., the model with all p predictors
        \item For $k = p, p-1, \ldots, 1$
        \begin{enumerate}
            \item Consider all $k-1$ models that keep all but one predictors in the current model $M_{k}$
            \item Among these, select the best model $M_{k-1}$ with the lowest error
        \end{enumerate}
        \item From the models $M_{0}, M_{1}, \ldots, M_{p}$, select the one with the lowest cross validation error on the evaluation choosing the appropriate error metric 
    \end{enumerate}

    The number of models explored is same as the forward stepwise method.\newline
    A hybrid approach is usually selected where we start with the usual forward selection method, but while adding variables, we do not add a variable if it does not give significant improvement. Another approach can be to remove redundant variables using p-test at every step of forward selection.

    
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsection{Metrics for evaluating Subset Models}
    In linear models, as we add more variables, the training error usually monotonically decreases. Test error may not behave in the same way. When training a model, the coefficients obtained are specific for minimizing the training error and hence will have less bias in comparison to the test error. \newline
    Hence, subset evaluation using training error will usually favour models with more number of variables. To overcome this
    \begin{itemize}
        \item Correct the training error estimate to correctly calculate test error
        \item Use a validation test or $k$-fold validation for better estimate of test error
    \end{itemize}
    
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsubsection{$C_{p}$ Estimate}
    For a least square fitted model,
    \begin{align*}
        C_{p} = \frac{1}{n}(RSS + 2p\hat{\sigma}^{2})
    \end{align*}
    $p$ is the number of predictors and $\hat{\sigma}^{2}$ is the estimate of the error associated with each observation. This is typically evaluated using the model built on all $p$ predictors.\newline
    Clearly, as $p$ increases, we are penalizing the model more to compensate for the decrease in the training RSS. When $\hat{\sigma}$ is an unbiased estimate of $\sigma$, we can show that this is infact an unbiased estimate of the test MSE.


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsubsection{Akaike Information Criterion (AIC)}
    AIC is defined for a large class of models fit by the maximum likelihood estimate.\newline
    For least squares fit in linear models, the errors are assumed to be gaussian and thus, AIC and least squares mean the same thing. For this case
    \begin{align*}
        AIC = \frac{1}{n\hat{\sigma}^2}(RSS + 2p\hat{\sigma}^2)
    \end{align*}
    where we have omitted an additive constant for the sake of simplicity.\newline
    For least squares models, $C_{p}$ and AIC are proportional to each other.


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsubsection{Bayesian Information Criterion (BIC)}
    BIC is derived from a Bayesian point of view, but ends up looking similar to the above defined errors.\newline
    For least squares error without constants, the BIC is
    \begin{align*}
        BIC = \frac{1}{n\hat{\sigma}^{2}} (RSS + \log(n)p\hat{\sigma}^{2})
    \end{align*}
    Note that the $\log(n)$ term will put a heavier weight on the error term for large $p$. Hence, BIC will tend to select models with lower number of variables in comparison to say $C_{p}$.

    
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsubsection{Adjusted $R^{2}$}
    Recall that $R^{2}$ is defined as $1 - RSS/TSS$. Adjusted $R^{2}$ is
    \begin{align*}
        Adjusted\;R^{2} = 1 - \frac{\frac{RSS}{n-p-1}}{\frac{TSS}{n-1}}
    \end{align*}
    This adjusted $R^{2}$ might increase or decrease when adding variables due to the terms corresponding to $p$. The intuition is that, after the correct number of variables have been identified, the decrease in RSS is less in comparison to the decrease in $n-p-1$ which will slightly increase the Adjusted $R^{2}$.


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsection{Shrinkage Methods}
    Instead of using a subset of predictors, we can also use all of the predictors and shrink the coefficients towards zero. This approach significantly reduces the variance in the model estimates. The famous ones here are \emph{Ridge Regression} and \emph{Lasso Regression}.

    
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsubsection{Ridge Regression}
    Ridge Regression is very similar to the least square estimate for linear regression except that we add a term corresponding to the squared sum of the regression coefficients in the error.
    \begin{align*}
        error &= RSS + \lambda \sum_{j=1}{p}\beta_{j}^{2}\\
              &= (Y-X\beta)^{T}(Y-X\beta) + \lambda \beta^{T}\beta
    \end{align*}
    $\lambda$ is a tuning parameter that needs to be chosen separately. It acts as a weight between the error in the data and how large are the regression coefficients. It is also known as the shrinkage penalty.\newline
    Note that we will not include the intercept term in shrinkage because it is simply the mean estimate of the model when all the predictors are zero and may not necessarily zero.\newline

    Using least squares estimate,
    \begin{align*}
        error &= (Y-X\beta)^{T}(Y-X\beta) + \lambda \beta^{T}\beta\\
        \frac{\partial error}{\partial \beta} &= 0\\
        \implies 0 &= -Y^{T}X - Y^{T}X + \beta^{T}X^{T}X + \beta^{T}X^{T}X + \lambda \beta^{T} + \lambda \beta^{T}\\
        \beta^{T}(X^{T}X + \lambda I) &= Y^{T}X\\
        (X^{T}X + \lambda I)^{T}\beta &= X^{T}Y\\
        (X^{T}X + \lambda I)\beta &= X^{T}Y\\
        \Aboxed{\beta &= (X^{T}X + \lambda I)^{-1}X^{T}Y}  
    \end{align*}
    $\lambda = 0$ will result in the simple least squares regression while $\lambda \to \inf$ will force the coefficients to go towards zero.\newline

    As is clear from the formula, the error term is sensitive to the actual scale of the coefficients which is ultimately dependent on the predictors themselves. In a simple least squares regression, the coefficients will scale up and down depending on how the data is scaled. The same is not true for Ridge Regression.\newline
    Hence when using \textbf{Ridge Regression, it is always advisable to \emph{standardize} the predictors} before training the model using
    \begin{align*}
        \tilde{x}_{ij} = \frac{x_{ij}}{\sqrt{\frac{1}{n}\sum_{i=1}{n}(x_{ij}-\bar{x}_{j})^{2}}}
    \end{align*}

    The success of Ridge Regression is based in the \textbf{bias variance tradeoff}. If the data is linear, simple linear regression will have a very low bias but high variance, making it sensitive to the training data. As $\lambda$ is introduced, it forces the model to have less flexibility by reducing the coefficiets value and subsequently their power on the prediction. This causes a reduction in the variance at expense of slight increase in bias. However, this trend is not monotonic with increasing $\lambda$ and the appropriate value must be chosen based on the errors observed. 

    
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsubsection{Lasso Regression}
    Notice that Ridge Regression will try to reduce the value of some of the coefficients, but it will never set them to exactly zero. Hence, we will end up using all the $p$ predictors in the model which may not be interpretable if the value of $p$ is large.\newline

    Lasso Regression comes over this disadvantage by defining the error as
    \begin{align*}
        error &= RSS + \lambda \sum_{j=1}^{p}\mid \lambda_{j} \mid
    \end{align*}

    When \textbf{$\lambda$ is sufficiently large, Lasso Regression forces some of the variables to be exactly zero}. This is very useful in reducing the subset of variables that we use in the model, thereby increasing model interpretability.


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsubsection{Alternative Formulation to Ridge and Lasso Regression}
    These regressions can also be considered as solving a constrained optimisation problem
    \begin{align*}
        &\minimize_{\beta}  \bigg\{\sum_{i=1}^{n} (y_{i} - \beta_{0} - \sum_{j=1}^{p}\beta_{j}x_{ij})^{2} \bigg\} \text{\;\;\;subject to\;} &&\sum_{j=1}^{p}\beta_{j}^{2} \leq s \\
        &\minimize_{\beta}  \bigg\{\sum_{i=1}^{n} (y_{i} - \beta_{0} - \sum_{j=1}^{p}\beta_{j}x_{ij})^{2} \bigg\} \text{\;\;\;subject to\;} &&\sum_{j=1}^{p}\mid \beta_{j} \mid \leq s
    \end{align*}
    for Ridge and Lasso regression respectively. This holds true because these regressions are effectively trying to limit the size of the coefficients themselves. For $\lambda = 0$, we have no bound on the size and $s$ in equations above is close to $\inf$. As $\lambda$ increases, $s$ will start to decrease and be $0$ for $\lambda \to \inf$. \newline

    The equations above can be interpreted as finding the minimum RSS among the points that lie inside the geometric shapes defined by the constraints. For $p = 2$, Ridge defines a circle $\beta_{1}^{2} + \beta_{2}^{2} \leq s$ and Lasso defines a diamond $\mid \beta_{1} \mid + \mid \beta_{2} \mid \leq s$.

    
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsubsection{Variable Selection Property of Lasso Regression}
    \begin{figure}[h]
    \includegraphics[scale=0.4]{lasso_contours}
    \centering
    \caption{For $p=2$, the left and right images correspond to Lasso and Ridge Regression.}
    \label{fig:lasso_contour} %\ref{fig:lasso_contour}
    \end{figure}

    In the figure \ref{fig:lasso_contour}, $\hat{\beta}$ corresponds to the least squares estimate of $\beta$ and the contours in red colour show the same value of RSS. The blue coloured regions correspond to the constraints defined above (diamond for lasso and circle for ridge).\newline
    Clearly, circle being a smooth shape, the lowest RSS contour will not usually touch it at one of the axis points. However, for the sharp diamond shape, the least RSS value is likely to be encountered along the axis. The shapes of the constraint regions can be controlled through $\lambda$ and for lasso, more constrained models (small $s$ or higher $\lambda$) will cause more of the coefficients to be zero. The argmuents are very well valid in higher dimensions as well.

    
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsubsection{Bayesian Interpretation}
    Lasso and Ridge Regression are also natural solutions when the the coefficients are assumed to have certain specific priors.\newline
    \begin{figure}[h]
        \includegraphics[scale=0.4]{lasso_bayes}
        \centering
        \caption {Gaussian prior on left and double exponential on the right}
        \label{fig:lasso_bayes}
    \end{figure}
    
    \begin{align*}
        p(\beta|X,Y) &\propto f(Y|X,\beta)p(\beta|X) = f(Y|X,\beta)p(\beta) \tag*{assuming X is constant}\\
        Y &= \beta_{0} + \beta_{1}X_{1} + \cdots + \beta_{p}X_{p} + \epsilon\\
        \text{Assume, } p(\beta) &= \prod_{j=1}^{p} \beta_{p} \tag*{for some density function $g$}  
    \end{align*}

    The following are observed for different priors on $\beta$

    \begin{itemize}
        \item When the density function of $\beta_{j}$ is assumed to be a standard normal, the posterior is same as solving the ridge regression error function
        \item When the density function is assumed to be a double exponential, the posterior is the same as solving lasso error function
    \end{itemize}

    From the visuals of the priors in figure \ref{fig:lasso_bayes}, double exponential is steeply peaked at zero, which clearly implies that the prior itself assumes some of the coefficients are likely zero. On the other hand, the gaussian priod is flatter and does not necessarily require the coefficients to be zero.


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsection{Dimension Reduction Methods}
    The methods seen above achieve reduction in variance by either reducing the number of variables being used for estimation, or reduce the value of coefficients of those variables.\newline
    Dimensionality reduction uses a \textbf{linear map} to convert the original $p$ variables to $M$ variables where $M < p$.
    \begin{align*}
        Z_{m} &= \sum_{j=1}^{p} \phi_{jm}X_{j} \tag*{where $\phi_{jm}$ are constants} \\
        RSS &= \sum_{i=1}^{n} (y_{i} - (\theta_{0}+\sum_{m=1}^{M}\theta_{m}Z_{m}))^{2} \tag*{where $\theta_{m}$ are linear regression coefficients for $Z_{m}$}
    \end{align*}
    This new formulation restructures the original least squares formula. The whole process now is
    \begin{itemize}
        \item Obtain the linear map using techniques like PCA
        \item Use least squares on the transformed predictors to obtain regression estimates
    \end{itemize}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsubsection{Principal Components Analysis (PCA)}
    PCA is one of the most common methods used for dimensionality reduction. It is based on the principle of finding those directions that \textbf{maximize the variance of data}. The intuition behind finding this direction is that this separates the data best given the high variance. Hence, the performance of a classifier/regressor would be better if the training was done using the transformed predictors. \newline

    Assume that we want to find $M$ components for PCA, where $M <= p$. Then,
    \begin{enumerate}
        \item Find the direction/projection of the data that gives the maximum variance under the constraint $\sum_{j=1}^{p}\phi_{jm}^{2} = 1$ (normalized vector) for obtaining the $m^{th}$ transformed predictor
        \item If the total components found is $< M$, repeat step 1 with the added constraint that the next component has zero correlation with the previous component
    \end{enumerate}
    This constraint of finding components with zero correlation essentially means that in the multidimensional space, we are finding a set of \textbf{orthogonal vectors}. The order of choosing the new predictors is in decreasing order of the information they contain. Thus, the first predictor will now containt the most information. \newline

    %%%%%%%%%%%%%%%
    \paragraph{Derivation of PCA}
    It is a good idea to \textbf{standardize the variables before running PCA}. Not only is it easier to derive the components, but they are also unaffected by the scale of the individual variables (since we know that scaling a variable multiplies the variance by square of the scaler and PCA is all about finding maximum variances !). \newline \newline
    After setting the the mean of all predictors to zero and standard deviation to one, the components can be derived by considering the following optimization problem
    \begin{gather*}
        \maximize_{\phi_{11},\ldots,\phi_{p1}} \big\{ \frac{1}{n} \sum_{i=1}^{n} (\sum_{j=1}^{p}\phi_{j1}x_{ij})^{2} \big\} \text{ subject to } \sum_{j=1}^{p} \phi_{j1}^{2} = 1\\
        \text{Or, } \maximize_{\Phi_{1}} \frac{1}{n} Z_{1}^{T}Z_{1} \text{ subject to } \sum_{j=1}^{p} \phi_{j1}^{2} = 1\\
        \text{Where } Z_{1} = X\Phi_{1}\\
        \text{Define the Lagrangian } L(\Phi_{1}, \lambda) = \frac{1}{n} (\Phi_{1}^{T}X^{T}X\Phi_{1}) - \lambda (\Phi^{T}\Phi - 1)\\
        \frac{\partial L(\Phi_{1}, \lambda)}{\partial \lambda} = 0 \text{, and } \frac{\partial L(\Phi_{1}, \lambda)}{\partial \Phi_{1}} = 0 \\
        \text{Giving, } 2\frac{1}{n}(\Phi_{1}^{T}X^{T}X) - 2\lambda \Phi_{1}^{T} = 0 \text{ and } \Phi_{1}^{T}\Phi_{1} - 1 = 0\\
        \frac{X^{T}X}{n}\Phi_{1} = \lambda \Phi_{1}\\
        \text{Since all } X_{i} \text{ are centered, } \frac{X^{T}X}{n} \text{ is the covariance matrix } \Sigma_{X}\\
        \Sigma_{X} \Phi_{1} = \lambda \Phi_{1} \text{ with } \Phi_{1}^{T}\Phi_{1} = 1\\
    \end{gather*}
    Which is nothing but the \textbf{Eigenvectors of the Covariance Matrix of $X$}. Notice that the maximization is achieved through the eigenvector with the highest eigenvalue. Since $\Sigma_{X}$ is positive semi-definite, all the eigenvalues will be $\geq 0$. \newline
    Furthermore, we know that all eigenvectors are orthogonal to each other, and hence they will also satisfy the condition that all the linear mapping vectors are not correlated to each other. \newline
    If we try to find the coefficients for $Z_{2}$, we are looking at the exact same optimization, but with the additional constraint that $\Phi_{2}$ is not correlated to $\Phi_{1}$. This will yield the second eigenvector of the covariance matrix. \newline
    Hence, \textbf{the linear maps for obtaining the PCA transformations are nothing but the eigenvectors of the covariance matrix $\Sigma_{X}$ of the original data $X$}. We have a total of $p$ eigenvectors and thus, the maximum components obtainable is also $p$.

    %%%%%%%%%%%%%%%
    \paragraph{Explained Variance}
    The variance explained by the $m^{th}$ component is nothing but the ratio of variance of $Z_{m}$ and total variance of the data. Mathematically this is
    \begin{align*}
        \text{Explained variance of component } m &= \frac{\frac{1}{n}\sum_{i=1}^{n} (\sum_{j=1}^{p} \phi_{jm} x_{ij})}{\frac{1}{n}\sum_{i=1}{n} \sum_{j=1}{p} x_{ij}^2}\\
                &= \frac{\frac{1}{n}Z_{m}^{T}Z_{m}}{tr(\frac{X^{T}X}{n})}\\
                &= \frac{\Phi_{m}^{T}\frac{X^{T}X}{n}\Phi_{m}}{tr(\Sigma_{X})}\\
    \end{align*}
    However, note that in the derivation, $\Phi_{m}$ is the eigenvector and thus, $(X^{T}X)\Phi_{m} = \lambda_{m}\Phi_{m}$ and $\Phi_{m}^{T}\Phi_{m} = 1$. Substituiting in the above equation,
    \begin{align*}
        \text{Explained variance } &= \frac{\lambda_{m}}{tr(\Sigma_{X})}\\
        \text{or, } \Aboxed{\text{Explained variance } &= \frac{\lambda_{m}}{\lambda_{1} + \cdots +\lambda_{p}}}
    \end{align*}
    Where the last equation comes from the fact that the trace of a matrix is simply the sum of it's eigenvalues.\newline
    A \textbf{Scree Plot} is a plot between the explained variance and the index of the prinicple components. It's cumulative version can be used to determine the number of components to keep on the basis of how much of the total variance we want to explain. The elbow point of the Scree Plot can also help determine the components at which the explained variance drops significantly.
    \paragraph{Principal Components Regression (PCR)}
    is simply using some $m$ out of $M$ components to perform linear regression. This approach will typically work best when a few components of the PCA sufficiently explain the whole data. This way we reduce the variance at the cost of slight change in bias. PCR can also be viewed as a continuous version of Ridge Regression.
    

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsubection{Partial Least Squares}
    This method is closely related to PCA/PCR. In the previous methods, an unsupervised approach was used to project the matrix $M$ onto a lower dimensional space. This transformation did not take $Y$, the response, into consideration. PLS will incorporate $Y$ as well for finding the transformations.\newline
    The algorithm is as follows
    \begin{enumerate}
        \item Calculate the coefficients for the first component as the coefficient obtained by regressing $Y$ on $X_{j}$
        \item Using this component, regress $X_{j}$'s on $Z_{1}$ and get the residuals. These are the unexplained components of the variables.
        \item Calculate $Z_{2}$ using $Y$ and these residuals
        \item Repeate the process above till $M$ components are obtained
    \end{enumerate}

    Though this procedure looks slightly more involved, it performs not better than lasso regression and PCR in practice. Although this method reduces bias, it also leads to quite an increase in variance as well, making the method not very useful.

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsection{Curse of Dimensionality}
    Most of the methods discuss here \textbf{work well when $n >> p$}. There can be many reasons why the model may not perform well in higher dimensions, but the major one will be the fact that as more and more dimensions are added to the model, chances of overfitting increase and so do the chances that the additional variables are simply noise.\newline
    We refer to the problem of training a model a high dimensional problem when $p > n$, or we have more data than number of predictors. Note that it is easy to obtain $R^{2} = 1$ in such a setting which consequently leads to $\hat{\sigma}^{2} \approx 0$. Metrics like $C_{p}$, AIC, BIC become useless.\newline
    Hence, in higher dimensional settings, it is important to obtain model performance on unseen data as there is a good chance of obtaining perfect results on the training set. Metrics associated with the training set can thus prove to be misleading.

    \paragraph{Intuition}
    for the curse of dimensionality can be easily obtained in the context of KNN.\newline
    Suppose the data is uniformly distributed along any dimension considered. Let the model be built in such a way that when making predictions, it uses $10\%$ of the data along all dimensions. \newline
    In the case of a single dimension, we need $10\%$ of the data. In the case of 10 dimensions, we will need to check $10\% ^{10} = 10^{-8}\%$ of data. Clearly this number grows as we consider more and more dimensions.\newline
    Thus, as the number of dimensions grow, we require substantially more data as the data observable in the neighborhood of the point is extremely small. This illustrates the fact that in higher dimensions, having less data will lead to a poor model.
\end{document}