\documentclass[../statistical_learning_notes.tex]{subfiles}
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Linear Regression}
Linear Regression is a parametric model where we assume a linear relationship between the dependent and independent variables.
\begin{align*}
    X &= (X_{1}, X_{2}, \ldots, X_{p})\\
    Y &= \beta_{0} + \beta_{1}X_{1} + \beta_{2}X_{2} + \cdots + \beta_{p}X_{p} + \epsilon
\end{align*}
Where $X$ represents a p dimensional input and $\beta$ are the coefficients, and $\epsilon$ is the error which is assumed to have $\mathcal{N}(0, \sigma^{2}$ distribution. Errors are assumed to be independent. We will usually not know the error or it's variance, and hence our estimate is denoted by $\hat{Y}$. Further, the estimted coefficients will also be denoted with a hat since we can never know the true model, but only get estimates of these parameters
\begin{align*}
    \hat{Y} &= \hat{\beta_{0}} + \hat{\beta_{1}}X_{1} + \beta_{2}X_{2} + \cdots + \hat{\beta_{p}}X_{p} + \epsilon
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Simple Linear Regression}
Here, the independent variable is assumed to have a single dimension, and thus we are only looking towards estimating two coefficients. Let $x_{i}$ be the $i^{th}$ observation and ${Y_{i}}$ be the associated response, then 
\begin{gather*}
    \hat{Y_{i}} = \hat{\beta_{0}} + \hat{\beta_{1}}x_{i}\\
    \text{Minimize error to estimate coefficients} \quad \minimize_{\beta_{0}, \beta_{1}} \sum_{i=1}^{n} (y_{i} - \beta_{0} - \beta_{1}x_{i})^{2}\\
    \hat{\beta_{1}} = \frac{\sum_{i=1}^{n}(x_{i} - \bar{x})(Y_{i} - \overline{Y})}{\sum_{i=1}^{n} (x_{i} - \bar{x})^{2}}, \quad \hat{\beta_{0}} = \overline{Y} - \hat{\beta_{1}}\bar{x}\\
    \hat{\theta}_{1} \sim \mathcal{N}\bigg(\theta_{1}, \frac{\sigma^{2}}{\sum_{i=1}^{n}(x_{i} - \overline{x})^{2}} \bigg)\\
    \hat{\theta}_{0} \sim \mathcal{N}\bigg(\theta_{1}, \sigma^{2} \frac{\sum_{i=1}^{n} x_{i}^{2}}{n\big((\sum_{i=1}^{n} x_{i}^{2}) - n\bar{x}^{2} \big)} \bigg)
\end{gather*}

The mean squared error is used here as it is the natural error function that emerges when we try to obtain MLE estimates of the coefficients. As is visible from the fomulae for point estimates of coefficients, they are linear combinations of normal variables ($Y$) and thus are normally distributed.\newline
Simple linear regression is discussed in detail in the \href{https://github.com/DragonWarrior15/statistical-learning-notes/blob/master/tex_files/probability/probability-notes.pdf}{probability notes}. It also discusses confidence intervals for the coefficients, the prediction intervals for the response and hypothesis testing for relation between input and response.\newline


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Coefficient of Determination}
$R^{2}$ is often used as a metric for checking how good the regression model is. It's definition is invariant to the number of independent variables
\begin{gather*}
    R^{2} = \frac{S_{YY} - RSS}{S_{YY}} = 1 - \frac{RSS}{S_{YY}}\\
    S_{YY} = \text{total variance in Y, }\quad RSS = \text{sum of squares of residuals}\\
    S_{YY} - RSS = \text{total variance explained by inputs}
\end{gather*}
A good model will explain most of the variance in $Y$ usig the input variables. Hence, $R^{2}$ close to $1$ is a good model and vice versa. \textbf{$R^{2}$ usually increases as more and more variables are added to the model}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Multiple Linear Regression}
By minimizing the sum squared error in a fashion similar to simple linear regression case, we can obtain the regression coefficients as follows ($X$ is $n \times p+1$ size with the first column containing all $1s)$
\begin{align*}
    \hat{Y} &= X\hat{\beta}\\
    MSE &= (Y-\hat{Y})^{T}(Y-\hat{Y})\\
    \frac{\partial}{\partial \hat{\beta}} MSE &= 0 \\
    \text{or,} \quad 0 &= \frac{\partial}{\partial \hat{\beta}} (Y^{T}Y - Y^{T}X\hat{\beta} - \hat{\beta}^{T}X^{T}Y + \hat{\beta}^{T}X^{T}X\hat{\beta})\\
    0 &= -Y^{T}X - Y^{T}X + \hat{\beta}^{T}X^{T}X + \hat{\beta}^{T}X^{T}X\\
    (X^{T}X)\hat{\beta} &= X^{T}Y\\
    \hat{\beta} &= (X^{T}X)^{-1}X^{T}Y \numberthiseqn\label{eq:linear_reg_solution}
\end{align*}

We note that the coefficients are linear combinations of $Y$ and thus normally distributed themselves.
\begin{alignat*}{1}
    E[\hat{\beta}] &= E[(X^{T}X)^{-1}X^{T}Y] = E[(X^{T}X)^{-1}X^{T}(X\beta + \epsilon)]\\
    &= E[\beta] + (X^{T}X)^{-1}X^{T}E[\epsilon]\\
    &= \beta\\
    \text{Since} \quad Cov(\mathbf{x}) &= E[(\mathbf{x} - E[\mathbf{x}])(\mathbf{x} - E[\mathbf{x}])^{T}]\text{,}\\
    \text{and} \quad Cov(\epsilon) &= E[(\epsilon - 0)(\epsilon - 0)^{T}] = E[\epsilon \epsilon^{T}] = \sigma^{2}\\
    Cov(\hat{\beta}) &= Cov((X^{T}X)^{-1}X^{T}Y)\\ &= E[((X^{T}X)^{-1}X^{T}(X\beta + \epsilon) - \beta)((X^{T}X)^{-1}X^{T}(X\beta + \epsilon) - \beta)^{T}]\\
    &= (X^{T}X)^{-1}X^{T}X(X^{T}X)^{-T}E[\epsilon \epsilon^{T}]\\
    &= (X^{T}X)^{-1}X^{T}\sigma^{2}\\
    \hat{\beta} &\sim \mathcal{N}(\beta, (X^{T}X)^{-1}\sigma^{2})
\end{alignat*}
$\hat{\beta}$ is a linear combination of independent $Y_{i}s$ and is normally distributed. Also, note that $X^{T}X$ is symmetric and so will be it's inverse.\newline

An unbiased estimator of $\sigma^{2}$ is
\begin{align*}
    \hat{\sigma}^{2} = \frac{1}{n-p-1} \sum_{i=1}^{n} (y_{i} - \hat{y}_{i})^{2} \text{,}\quad E[\hat{\sigma}^{2}] = \sigma^{2}
\end{align*}
where the denominator is chosen to make the estimator unbiased. it can be shown that
\begin{align*}
    (n - p - 1) \frac{\hat{\sigma}^{2}}{\sigma^{2}} \sim \chi_{n-p-1}^{2}
\end{align*}

with $\hat{\beta}$ and $\hat{\sigma}^{2}$ independent random variables.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Confidence Intervals}
The mean response for any new input is also a random variable with the distributions
\begin{align*}
    E[x^{T}\hat{\beta}] &= x^{T}\beta\\
    Var(x^{T}\hat{\beta}) &= E[(\hat{\beta} - \beta)(\hat{\beta} - \beta)^{T}]\\
    &= x^{T}(E[\hat{\beta}\hat{\beta}^{T}] - \beta\beta^{T})x = x^{T}E[(\hat{\beta} - \beta)(\hat{\beta} - \beta)^{T}]x\\
    &= x^{T}(X^{T}X)^{-1}x \sigma^{2}\\
    x^{T}\hat{\beta} &\sim \mathcal{N}(x^{T}\beta, x^{T}(X^{T}X)^{-1}x \sigma^{2})
\end{align*}

Utilizing the unbiased estimate of $\sigma^{2}$ defined above, we can get the confidence and prediction intervals as follows (using t-distribution after dividing the normal distribution of response with the estimator of $\sigma^{2}$) for $1-\alpha$ confidence
\begin{alignat*}{2}
    \text{confidence interval for mean response} \quad &x^{T}\beta \quad &\in \quad &x^{T}\hat{\beta} \pm t_{\alpha/2, n-p} \sqrt{x^{T}(X^{T}X)^{-1}x \sigma^{2}}\\
    \text{prediction interval for response} \quad &Y \quad &\in \quad &x^{T}\hat{\beta} \pm t_{\alpha/2, n-p} \sqrt{1 + x^{T}(X^{T}X)^{-1}x \sigma^{2}}
\end{alignat*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Hypopthesis Testing}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Single Coefficient}
To test
\begin{align*}
    H_{0}: \beta_{j} = 0 \quad \text{versus} \quad H_{1}: beta_{j} \neq 0
\end{align*}
we utilize some of the above defined distributions
\begin{gather*}
    \hat{\beta}_{j} \sim \mathcal{N}(\beta_{j}, \sigma^{2}diag((X^{T}X)^{-1})_{j})\\
    (n - p - 1) \frac{\hat{\sigma}^{2}}{\sigma^{2}} \sim \chi_{n-p-1}^{2}\\
    \frac{\hat{\beta}_{j} - \beta_{j}}{\sigma\sqrt{diag((X^{T}X)^{-1})_{j}}} \div \sqrt{(n - p - 1) \frac{\hat{\sigma}^{2}}{\sigma^{2}(n-p-1)}} \sim t_{n-p-1}\\
    \text{and under null hypothesis,} \quad \frac{\hat{\beta}_{j}}{\hat{\sigma}\sqrt{diag((X^{T}X)^{-1})_{j}}} \sim t_{n-p-1}\\
    \text{where} \quad \frac{\hat{\beta}_{j}}{\hat{\sigma}\sqrt{diag((X^{T}X)^{-1})_{j}}} \quad \text{is often called \emph{Z-score}}
\end{gather*}
where variance utilizes the diagonal entry of variance of $\beta$. If we know the actual variance $\sigma^{2}$, we replace it in the above equation to get a normal distribution instead. A large value of the \emph{Z-score} will lead to the elimination of the null hypothesis meaning the coefficient is not zero. The $1-\alpha$ confidence intervals for $\beta_{j}$ then become
\begin{gather*}
    \beta_{j} \in \hat{\beta} \pm \hat{\sigma} t_{\alpha/2, n-p-1} \sqrt{diag((X^{T}X)^{-1})_{j}}
\end{gather*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Group of Coefficients, F-test}
Suppose we have a set of $k$ coefficients for a categorical variable and we wish to test
\begin{align*}
     H_{0}: \beta_{i} = \beta_{i+1} = \cdots = \beta_{k} = 0 \quad \text{versus} \quad H_{1}: \text{at least one of} \quad \beta_{j} \neq 0, j \in (i, \ldots, k)
\end{align*}
Then, we use the \textbf{F-test} assuming $H_{0}$ is true
\begin{gather*}
    F = \frac{(RSS_{0} - RSS_{1})/k}{RSS_{1}/(n-p-1)}\\
    \text{where} \quad RSS_{0} = RSS \quad \text{of model without the $k$ coefficients}\\
    \text{and} \quad RSS_{1} = RSS \quad \text{of model with all the coefficients}\\
    F \sim F_{k, n - p - 1}
\end{gather*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Multiple Outputs}
We want to predict multiple outputs $Y_{1}, Y_{2}, \ldots, Y_{k}$ from the same set of variables. The $RSS$ then becomes
\begin{gather*}
    RSS = \sum_{i=1}^{n} \sum_{j=1}^{k} (y_{i,j} - \hat{y}_{i,j})^{2} = \sum_{i=1}^{n} \sum_{j=1}^{k} (y_{i,j} - \beta_{k,0} - \sum_{u=1}^{p} \beta_{k,u}x_{i,u})^{2}\\
    \text{Minimizing,} \quad \hat{\beta} = (X_{T}X)^{-1}X^{T}Y \quad \text{where $Y$ is a $n \times k$ matrix}
\end{gather*}
Thus, the problem is similar to doing the linear regression independently on each of the $Y_{j}s$. One important assumption here is that the errors between different $Y_{j}$ are not correlated with each other.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Coefficient Interpretation}
The interpretation of coefficients for quantitative variables is straightforward. Further, suppose the equation has the form
\begin{align*}
    y = \beta_{0} + \beta_{1}X_{1} + \beta_{2}X_{2} + \ldots + \beta_{p}X_{p} + \epsilon
\end{align*}

Then, $\beta_{1}$ denotes the change in $y$ that will be caused by changing the value of $X_{1}$ by 1 unit, provided all other inputs are constant. If we change two variables simultaneously, we can see their interaction together keeping the remaining variables constant.\newline

However, this procedure is not straightforward for qualitative/categorical variables. Suppose we have one numeric variable, say age, and a variable denoting gender which we will constraint to have two values, $0$ denoting male and $1$ denoting female. The regression equation becomes
\begin{align*}
    y =\beta_{age}x_{age} + \beta_{gender}x_{gender} + \epsilon = \begin{cases}
        \beta_{age}x_{age} + \beta_{gender} + \epsilon \quad &\text{if male}\\
        \beta_{age}x_{age} + \epsilon \quad &\text{otherwise}
    \end{cases}
\end{align*}

Thus, $\beta_{gender} (<0)$ signifies how much $y$ is less for males compared to females. The coefficient in itself has no meaning unless compared with respect to a base value.\newline

Suppose we were to change the convention to $-1$ for females and $1$ for males, then
\begin{align*}
    y =\beta_{age}x_{age} + \beta_{gender}x_{gender} + \epsilon = \begin{cases}
        \beta_{age}x_{age} + \beta_{gender} + \epsilon \quad &\text{if male}\\
        \beta_{age}x_{age} -\beta_{gender} + \epsilon \quad &\text{otherwise}
    \end{cases}
\end{align*}

Now, $2\beta_{gender}$ gives us the difference between the value of $y$ between males and females, and $\beta_{gender}$ denotes the change on either side from the base value of $\beta_{age}x_{age} + \epsilon$. This new $\beta$ should be have of the original coefficient because the real relation has stayed the same. Thus, the interpretation of coefficients changes based on how the variable gets defined.\newline

In case of $n$ levels, we will create $n-1$ variables (since the last variable is perfectly correlated with all the remaining ones). Then, the coefficient of the $i^{th}$ level is simply the increment over the base (last) level. Note that there is no coefficient for the last level, and all the other coefficients are relative to this level.\newline

\paragraph{Hierarchical Principle} If we include an interaction in the model, we should include all the main effects, even if their \emph{p-values} of those coefficients are insignificant. Simply put, if $X_{1}X_{2}$ appears in the model, $X_{1}$ and $X_{2}$ should also be there. Similarly, for a categorical variable, either all the categories are present in the model, or none of them are present. It is alright to just use a single category, but then the remaining categories together constitute the base cateogory and in essence, all categories are still present in the model, albiet in a different form.\newline

\paragraph{Importance of interactions} Suppose we build a bank balance model dependent on income, and if a person is a student or not. Basic model
\begin{align*}
    y = \beta_{income}+x_{income} + \beta_{student}x_{student} + \epsilon
\end{align*}
The problem with this model is that for both student and non student, the effect of income is same, which is not what we want. Including interaction terms,
\begin{align*}
    y = \beta_{income}+x_{income} + \beta_{student}x_{student} + \beta_{income \times student}x_{income}x_{student} + \epsilon
\end{align*}
which gives different dependence on income (slope) for student ($\beta_{income} + \beta_{income \times student}$) and non student $(\beta_{income})$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problems when using Linear Regression}
\paragraph{Non linearity of data} will make linear regression perform poorly as the basic assumption is that the data has linear relation with response. In case the data is non-linear, we expect to see distinct patterns in the residual vs variable plots. To induce linearity, transformations like $\log$, $exp$, $sqrt$ etc. can be checked.

\paragraph{Correlation of errors} We do not expect $\epsilon_{i}$ to give any information regarding $\epsilon_{i+1}$ as that will cause the standard errors to be underestimated giving wider confidence intervals. This can be checked by plotting the residuals vs a shifted version, which should not give any discernable patterns.

\paragraph{Heteroscedasticity} or non-constant variance in residuals also violates the assumption that the response/errors have constant variance. This can be checked by plotting the residuals vs a variable. In case the variance is not constant, we expect to see the residuals to get further away from each other as we progress along the variable x.

\paragraph{Outliers} can cause trouble with the model as mean square error will give more weightage to larger errors. This can be checked for by plotting either the histogram of the variable to ensure thin tails, or by plotting the residual/standard error estimate, which should have no discernable pattern.

\paragraph{Multicollinearity} occurs when one of the variable is expressible as a linear combination of a set of the remaining variables. This can cause large variance in the estimation of the coefficients. It can be checked for using \emph{Variance Inflation Factor}
\begin{align*}
    VIF &= \frac{1}{1 - R_{X_{j}|X_{-j}}^{2}}
\end{align*}
where $R_{X_{j}|X_{-j}}^{2}$ is the $R^{2}$ score obtained by regressing the $j^{th}$ variable on all the remaining variables. Typically, VIF should be $\leq 3-5$. Higher values indicate significant correlation of this variable with the others.
\end{document}
