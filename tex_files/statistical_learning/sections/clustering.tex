\documentclass[../statistical_learning_notes.tex]{subfiles}
\begin{document}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \chapter{Clustering}
    Clustering is an unsupervised learning techniques that aims to finds clusters or groups in the data such that observations in the same group are similar to each other while observations in different groups are different from each other.\newline

    Clustering is useful for an exploratory analysis of the data and also useful in problems like customer segmentation. It comes in three flavours
    \begin{itemize}
        \item \textbf{Combinatorial Algorithms} : such algorithms work using the provided data, without assuming any underlying probability distribution.
        \item \textbf{Mixture Modelling} : such algorithms assume the data to be independently and identically distributed from a probability distribution whose parameters are unknown. The same distribution with different parameters is assumed for each of the clusters, and the intent is to find the parameters through the data, usually using Maximum Likelihood.
        \item \textbf{Mode Seeking Algorithms} : these algorithms aim to find modal points in the data where the density of the points is highest. Observations close to the mode define the cluster. PRIM is a fine example of such bump hunting algorithm.
    \end{itemize}

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Distances and Dissimilarity Measures}
    Clustering aims to group observations similar observations in the same group, while dissimilar observations fall in different groups. To achieve this mathematically, we need to define a way to measure dissimilarity between observations. If we have $N$ observations with $p$ variables, then
    \begin{align*}
        D(x_{a}, x_{b}) = \sum_{j=1}^{p}d_{j}(x_{aj}, x_{bj})
    \end{align*}

    where $D$ is a symmetric similarity matrix of size $N \times N$. To get this for any pairs of values, we sum up the similarity measure along all the variables. Most algorithms need this matrix to be symmetric and if that is not the case, replace $D$ with $(D + D^{T})/2$ will help.\newline

    The most common choice for $d_{j}$ is squared distance, but it can also be replaced with the absolute difference. The desirable properties of any such measure is that the values should be positive and monotonically increasing.

    \paragraph{Quantitative Variables} are the simplest to deal with. We can define a real valued monotonically increasing function in several possible ways
    \begin{align*}
        d_{j}(x_{aj}, x_{bj}) &= \lvert x_{aj} - x_{bj} \rvert\\
        d_{j}(x_{aj}, x_{bj}) &= (x_{aj} - x_{bj})^{2}\\
        D(x_{a}, x_{b}) &= \frac{\sum_{j=1}^{p} (x_{aj} - \overline{x}_{a})(x_{bj} - \overline{x}_{b})}{\sqrt{\sum_{j=1}^{p} (x_{aj} - \overline{x}_{a})^{2}} \sqrt{\sum_{j=1}^{p} (x_{bj} - \overline{x}_{b})^{2}}}, \quad \overline{x}_{a} = \frac{1}{p}\sum_{j=1}^{p} x_{aj}
    \end{align*}

    If we have standardized the observations, the last equation will simply be similar to squared distance, $\sum_{j=1}^{p} (x_{aj}-x_{bj})^{2} \propto 2(1 - \rho(x_{a}, x_{b}))$. Hence clustering based on correlation will be similar to the one based on squared distances.\newline
    Correlation based measure can be useful in retail behaviour when we want to check profiles based on the whether similar products are purchased rather than how many of them are purchased.

    \paragraph{Ordinal Variables} already have a pre defined ranking order. If we number such variable with $M$ levels from $1$ to $M$, the levels are replaced with
    \begin{align*}
        \frac{i - 1/2}{M}, \quad i = 1, 2, \ldots, M
    \end{align*}
    and treat them simply as a numeric variable.

    \paragraph{Categorical Variables} are handled in a slightly different manner. If the variable has $M$ levels, we create an $M \times M$ matrix $L$ such that $L{ij}$ is the required measure of dissimilarity between $i^{th}$ and $j^{th}$ variables. A common choice is to have all entries of $L$ equal 1 except the diagonals which are filled with 0. This gives equal weightage to all pairings.\newline

    For combining the different distance measures of the variables, we can use \\$D(x_{a}, x_{b}) = \sum_{j=1}^{p} w_{j} d_{j}(x_{aj}, x_{bj})$ with $\sum_{j=1}^{p} w_{j} = 1$ to get a weighted convex combination of the measures. Note that if the different variables are on different scales (different variances), the individual contributions will not be $w_{j}$ but rather $w_{j} Var(X_{j})$. Thus, setting $w_{j} \propto 1/Var(X_{j})$ will enable each variable getting the same weight.\newline

    In case of missing values, it is often the case that those observations are simply ignored. For categorical variables, we usually treat them as a separate category. The imputation strategy for numerical variables will vary among problems and depend on the context.

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{K-means Clustering}
    A very powerful technique that organizes the data into $K$ distinct groups such that each observation will fall into exactly one group and when all the groups are combined, they cover the entire data set. $K$ is determined beforehand and is the number of clusters we are going to make.\newline

    The fundamental idea of clustering is to reduce the within cluster variation. Let $C_{k}$ denote the set containing the indices of the points falling in the cluster $k$ and $W(C_{k})$ be the within cluster variation for cluster $k$. Then,
    \begin{align*}
        \minimize_{C_{1},\ldots,C_{K}} \sum_{k=1}^{K} W(C_{k})
    \end{align*}

    Using Euclidean distance as a measure of the intercluster distance between two points, we can redefine the optimization summing across all the dimensions of the data as
    \begin{align*}
        \minimize_{C_{1},\ldots,C_{K}} \sum_{k=1}^{K} \frac{1}{\vert C_{k} \vert} \bigg\{\sum_{i_{1}, i_{2} \in C_{k}} \sum_{j=1}^{p} (x_{i_{1}j} - x_{i_{2}j})^{2} \bigg \}
    \end{align*}
    where $\vert C_{k} \vert$ is the number of observations in the cluster $C_{k}$. Considering all possible partitions is impossible for large $n$ and we use the following algorithm to obtain the local optimum.


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsection{Algorithm}
    We repeat the following until some predefined convergence criteria
    \begin{enumerate}
        \item Randomly assign a cluster in $1, \ldots, K$ to each observation in the data.
        \item Repeat the following till convergence
        \begin{enumerate}
            \item Calculate the centroid for each cluster where centroid is a $p$ dimensional vector whose each component is the average of the components of all the points that fall in the considered cluster.
            \item Assign each observation the cluster index of the centroid that is closest to the given observation (using Euclidean distance).
        \end{enumerate}
    \end{enumerate}

    \textbf{It is usually a good idea to center and standardize the variables first} so that the individual magnitudes and variances don't affect the Euclidean distances drastically.

    To see why using distance from centroid is a good replacement for the pairwise distance, consider the following equation
    \begin{align*}
        \sum_{i_{1}, i_{2} \in C_{k}} (x_{i_{1}} - x_{i_{2}})^{T} (x_{i_{1}} - x_{i_{2}}) = \frac{1}{2} \sum_{i \in C_{k}} \sum_{j \in C_{k}} (x_{i} - x_{j})^{T}(x_{i} - x_{j})
    \end{align*}
    where the right side allows for all possible pairs including the ones where the indices might repeat. Continuing to expand the right hand side,
    \begin{align*}
        \sum_{i \in C_{k}} \sum_{j \in C_{k}} (x_{i} - x_{j})^{T}(x_{i} - x_{j}) &= \sum_{i \in C_{k}} \bigg(\vert C_{k} \vert (x_{i}^{T} x_{i}) - 2x_{i}^{T} \sum_{j \in C_{k}} x_{j} + \sum_{j \in C_{k}} x_{j}^{T} x_{j} \bigg)\\
        &= \sum_{i \in C_{k}} \bigg( \vert C_{k} \vert (x_{i}^{T} x_{i}) - 2\vert C_{k} \vert x_{i}^{T} \bar{x} + \sum_{j \in C_{k}} x_{j}^{T} x_{j} \bigg)\\
        &= \sum_{i \in C_{k}} \bigg( \vert C_{k} \vert (x_{i}^{T} x_{i}) - 2\vert C_{k} \vert x_{i}^{T} \bar{x} \bigg) + \sum_{i \in C_{k}} \vert C_{k} \vert x_{i}^{T} x_{i}\\
        &= \sum_{i \in C_{k}} \bigg( 2\vert C_{k} \vert (x_{i}^{T} x_{i}) - 2\vert C_{k} \vert x_{i}^{T} \bar{x} \bigg)\\
        &= 2 \vert C_{k} \vert \bigg\{ \bigg( \sum_{i \in C_{k}}  (x_{i}^{T} x_{i}) \bigg) - \vert C_{k} \vert \bar{x}^{T} \bar{x} \bigg\}\\
        &= 2 \vert C_{k} \vert \bigg\{ \bigg( \sum_{i \in C_{k}}  (x_{i}^{T} x_{i}) \bigg) - 2\vert C_{k} \vert \bar{x}^{T} \bar{x} + \vert C_{k} \vert \bar{x}^{T} \bar{x} \bigg\}\\
        &= 2 \vert C_{k} \vert \bigg\{ \bigg( \sum_{i \in C_{k}}  (x_{i}^{T} x_{i}) \bigg) - 2 (\sum_{i \in C_{k}} x_{i}^{T} \bar{x}) + \vert C_{k} \vert \bar{x}^{T} \bar{x} \bigg\}\\
        &= 2 \vert C_{k} \vert \bigg\{ \sum_{i \in C_{k}}  x_{i}^{T} x_{i} - 2 x_{i}^{T} \bar{x} + \bar{x}^{T} \bar{x} \bigg\}\\
        &= 2 \vert C_{k} \vert \bigg\{ \sum_{i \in C_{k}}  (x_{i} - \bar{x})^{T} (x_{i} - \bar{x}) \bigg\}\\
        \text{Thus, } \frac{1}{\vert C_{k} \vert} \bigg\{\sum_{i_{1}, i_{2} \in C_{k}} \sum_{j=1}^{p} (x_{i_{1}j} - x_{i_{2}j})^{2} \bigg \} &= \bigg\{ \sum_{i \in C_{k}}  (x_{i} - \bar{x})^{T} (x_{i} - \bar{x}) \bigg\}
    \end{align*}

    Thus, the quantity we set out to minimize for each cluster is indeed the sum of distance of each point from the centroid of the cluster, which means the cluster is to be chosen based on the closest centroid to minimize the overall intra cluster distance.\newline

    \textbf{The optimum found by K-means clustering is local which makes it important to run the algorithm with different random initializations to get the minima}.\newline

    \textbf{Elbow Curve} is a plot between the total intra cluster distance vs the number of cluster $K$ and is a visual method to obtain the optimum number of clusters to use. The elbow shape refers to the fact that if there is indeed clusters present in the data, the plot will see a sharp decline in the intra cluster distance for some $k$.\newline
    Note that the curve is going to always keep decreasing as in the limiting case when we have $n$ points and $n$ clusters, the total distance will be zero. Hence, the number of clusters must be carefully chosen.\newline

    K-Means has a few problems when working with a dataset. Firstly, it requires all data to be numeric, and the distance metric used is the squared distance. Hence, the algorithm lacks robustness and is sensitive to outliers. Hence, it is worthwhile to explore other clustering strategies and dissimilarity measures that better suit the data


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{K-Mediods Clustering}
    This method is very similar to K-Means with the difference that the cluster centres are restricted to be one of the data points in the training set. Thus, the algorithm becomes
    \begin{enumerate}
        \item Randomly select $K$ points as the cluster centres, and assign every observation in the training set to the cluster with the closest centre.
        \item For each cluster, find the point in that cluster that has the minimum total cluster distance to all the other points in the cluster.
        \item Repeat steps 1 and 2 till convergence.
    \end{enumerate}

    The restriction of cluster centre being an oberved data point increases the computational cost because now we do not have a direct solution of the next cluster centre, but must rather iterate through all the points in the cluster.\newline

    This restriction gives rise to the property that while working with this algorithm, if we have the matrix corresponding to the dissimilarity measures, we do not need to know the individual attributes since we are not using those in the computations anymore. All the steps of the algorithm only require us to keep track of the indices that are cluster centres and the cluster assigned to each index. The dissimilarity measure can be directly used to compute the next cluster centre/cluster assignment.


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Hierarchical Clustering}
    $K$-means suffers from the disadvantage that the number of clusters needs to be specified beforehand. Hierarchical does not require such a consideration beforehand. here we dicsuss the \textbf{bottom-up} or \textbf{agglomerative clustering} approach. Hierarchical clustering is visualized using a \textbf{dendogram} which is a tree like diagram draw upside down. Starting from the bottom, branches are originate from the individual data points and slowly start merging as we move upward. The earlier the branches merge, the similar the data points are and vice versa. (Be careful to not judge the similarity from the proximity on the horizontal axis)\newline

    \begin{figure}[h]
    \includegraphics[scale=0.5]{dendogram}
    \centering
    \caption{Visualization of dendogram. The two curves on the right colour the different clusters obtained based on the height at which we decide to cut.}
    \label{fig:dendogram} %\ref{fig:dendogram}
    \end{figure}

    The number of clusters is simply determined by the height at which we made the cut. The middle figure in figure \ref{fig:dendogram} shows a cut at height 9 which results in two branches and thus two clusters.\newline
    Changing the height from the highest value to the lowest value will result in 1 and $n$ clusters respectively. Thus, we do not need to specify the number of clusters beforehand but it is rather to be chosen by us based on the height. This height can be seen similar to $K$ in $K$-means clustering.\newline

    The inherent nesting of the clusters as indicated visually by the dendogram may not be possible in every data set and there will be cases when $K$-means clustering may be superior.


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsection{Algorithm}\label{sec:gmm_algo}
    Hierarchical Clustering is performed in a bottom-up approach. Start with $n$ clusters where each observation is it's own cluster. Define a dissimilarity measure between each pair of observation. This can be Euclidean distance as well. Now, cluster the observations that are least dissimilar into the same group, which will give us $n-1$ clusters. Again use the dissimilarity measure to group two similar observations until the total number of clusters is 1.\newline

    Consequently, there will be cases when we need to determine the dissimilarity between a group and an observation or a pair of groups. This is done using \textbf{linkage}. Four types of linkages used are \textbf{complete, average, single} and \textbf{centroid}. Average and complete linkages are preferred over single linkages, and all three are more popular than centroid linkage. Average and complete likages will usually give balanced dendograms.\newline

    Following are teh descriptions of individual types of linkages
    \begin{itemize}
         \item Complete\newline
         Maximal intercluster dissimilarity. Take the maximum of the pairwise dissimilarity between observations of cluster A and cluster B.
         \item Single\newline
         Minimum intercluster dissimilarity. Take the minimum of the pairwise dissimilarity between observations of cluster A and cluster B.\newline
         Single linkage can result in extended trailing clusters in which single observations fuse one at a time.
         \item Average\newline
         Mean intercluster dissimilarity. Take the average of the pairwise dissimilarity between observations of cluster A and B.
         \item Centroid\newline
         Take the dissimilarity between the centroid of cluster A and B. Centroid linkages can result in undesirable inversions.
    \end{itemize}


    \begin{figure}[h]
    \includegraphics[scale=0.5]{dendogram_linkages}
    \centering
    \caption{Visualization of dendograms obtained for different choice of linkages.}
    \label{fig:dendogram_linkages} %\ref{fig:dendogram_linkages}
    \end{figure}

    The algorithm can be summarized as follows
    \begin{enumerate}
        \item Start with $n$ clusters where each observation is it's own cluster. Compute $n(n-1)/2$ pairwise dissimilarity measures between all pairs.
        \item For $i=n, n-1, \ldots, 2$
        \begin{enumerate}
            \item Compute the pairwise dissimilarity between all $i$ clusters and take the two clusters with the least dissimilarity (or highest similarity). Fuse them into a single cluster. The dissimilarity measure is also indicative of the height in the dendogram where the two clusters fuse.
            \item Recompute the pairwise dissimilarity between the $n-1$ clusters.
        \end{enumerate}
    \end{enumerate}

    The above is the case of agglomerative clustering which is a bottom up appraoch. Top down approaches are called divisive clustering, since they will recursively split observations into two clusters such that the inter cluster dissimilarity is the highest.


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Gaussian Mixture Models (GMM)}
    GMM is a soft assignment method where we dont assign a cluster to each data point, rather we know the relative probabilities of the point belonging to individual clusters. Suppose there are $K$ clusters and $\theta$ denote the common set of parameters for $K$ independent gaussian distributions, then,
    \begin{gather*}
        p(x_{i}|\theta) = \sum_{j=1}^{K} \pi_{j} \big( \mathcal{N}(\mu_{j}, \sigma_{j}^{2}) \; at \; x_{i} \big)\\
        \text{with,} \quad \sum_{j=1}^{K} \pi_{j} = 1, \quad \pi_{j} \geq 0, \quad \Sigma_{j} > 0\\
        \theta = \{ \pi_{1}, \ldots, \pi_{K}, \mu_{1}, \ldots, \mu_{K}, \Sigma_{1}, \ldots, \Sigma_{K} \}
    \end{gather*}
    where $\pi$ are probabilities of a point belonging to a particular cluster, and is constant for the whole data set. This implies that to calculate the likelihood of each data point, we are using a weighted sum of all the gaussians.\newline
    The additional constraint on covariance matrices simply means that they are positive semi definite, which is a necessary condition for invertability, and we know that the inverse is used in the probability distribution of a mutivariate normal.\newline

    The parameters can be found by maximizing the log likelihood of the data
    \begin{align*}
        \max_{\theta} \prod_{i=1}^{N}\sum_{j=1}^{K} P(x_{i}|\text{cluster} = j)P(\text{cluster} = k)\\
        \text{subject to} \quad \sum_{j=1}^{K} \pi_{j} = 1, \quad \pi_{j} \geq 0, \quad \Sigma_{j} > 0
    \end{align*}
    which is often difficult to perform due to the complexity of the distribution. When we take log on both sides, we will still have a summation inside the log term making optimization difficult.\newline

    
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsection{The Algorithm}
    To solve the problem of GMMs in a tractable way, we formulate a latent variable $t_{i}$ for each data point that indicates which cluster the point came from. This will be the same as $\pi_{j}$ defined earlier
    \begin{gather*}
        p(t = j|x_{i}, \theta) = \pi_{j}, \quad \sum_{j=1}^{K} p(t = j|x_{i}, \theta) = 1\\
        p(x_{i} | \theta) = \sum_{j=1}^{K} p(x_{i} | t = j, \theta) p(t = j|x_{i}, \theta)
    \end{gather*}
    The last equation formulates the original equations in a conditional probability format.\newline
    \subsubsection*{Calculating parameters when cluster assignments are known}
    Suppose we already knew the cluster assignments of each of the points, then we could simply calculate the mean of the gaussians as
    \begin{align*}
        \mu_{j} = \frac{\sum_{i:t_{i} = j} x_{i}}{\sum_{i=1}^{N} I(t_{i} = j)}  
    \end{align*}
    which is nothing but the MLE estimate of a gaussian distribution (assuming we know the points which fall in this particular gaussian). In our case, we do not know the hard assignments, but rather the soft assignments. Hence, we modify the above formula to include all points, but weighted by the cluster assignment probabilities
    \begin{align*}
        \mu_{j} = \frac{\sum_{i=1}^{N} p(t_{i} = j|x_{i}, \theta) x_{i}}{\sum_{i=1}^{N} p(t_{i} = j|x_{i}, \theta)}
    \end{align*}

    \subsubsection*{Calculating cluster assignment when parameters are known}
    If we know the parameters of the gaussian already, we can derive the cluster assignmens as
    \begin{gather*}
        p(t_{i} = j|x_{i}, \theta) \propto \mathcal{N}(\mu_{j}, \Sigma_{j}) \; at \; x_{i}\\
        \sum_{j=1}^{K} p(t_{i} = j|x_{i}, \theta) = 1
    \end{gather*}
    where the last equation helps in normalization to get valid probability distributions.\newline
    We can notice that we have landed in a cyclic problem. Given the cluster assignments, we can always calculate the gaussian parameters, and given the gaussian parameters, cluster assignments can be calculated.\newline
    Hence, to solve the problem of estimating the GMM parameters, we break it in two steps, similar to Expectation Maximaztion (EM) Algorithm
    \begin{enumerate}
        \item Assume a random value for the parameters of the gaussians (with all $\Sigma_{j} > 0$)
        \item Repeat until convergence of likelihood
        \begin{enumerate}
            \item Calculate cluster assignments for all points using
            \begin{align*}
                p(t_{i} = j|x_{i}, \theta) \propto \mathcal{N}(\mu_{j}, \Sigma_{j}) \; at \; x_{i}, \quad \sum_{j=1}^{K} p(t_{i} = j|x_{i}, \theta) = 1
            \end{align*}
            \item Calculate gaussian parameters using the above cluster assignments
            \begin{align*}
                \mu_{j} = \frac{\sum_{i=1}^{N} p(t_{i} = j|x_{i}, \theta) x_{i}}{\sum_{i=1}^{N} p(t_{i} = j|x_{i}, \theta)}
            \end{align*}
        \end{enumerate}
    \end{enumerate}

    This is a simpliefied version of EM algorithm. EM is known to give local optima (global optima is NP hard) and hence the results will vary with different initializations. Advisable to try and check different initializations.


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Key Considerations while Clustering}
    Following are a set of general rules when doing clustering
    \begin{itemize}
        \item centering and bringing the variables to the same scale is useful when measuring Euclidean distance for the obvious reason of not letting magnitudes affect the distances.
        \item Different types of clustering approaches should be explored to check which performs the best. This is important as in unsupervised learning, the structure of data is not known beforehand and it is important to explore multiple hypothesis.
        \item Several choices of similarity measures and linkages can be explored for further understanding of data.
        \item Clustering can be non robust and thus the results should be "validated" by performing clustering on multiple subsets of data to assure stability.
        \item In come cases, the hard cluster assignment of $K$-means and hierarchical clustering may not be useful. Probabilistic models like mixture models can be explored in this case.
    \end{itemize}
\end{document}