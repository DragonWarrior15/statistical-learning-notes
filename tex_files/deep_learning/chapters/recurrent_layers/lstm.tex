\documentclass[../../deep_learning_notes.tex]{subfiles}
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Long Short Term Memory (LSTM)}
This more complicated architecture learns when to remember and when to forget, and builds upon the concept of gate to restrict/allow flow of information, and we have an additional cell state that serves as the unit's memory. It contains three gates
\begin{enumerate}
    \item Forget Gate: Decides what information to keep/forget
    \item Input Gate: Decides how much of the new informaiton should be added
    \item Output Gate: Decides how much to chages should be done to get next hidden state
\end{enumerate}
Mathematically, we have the following sequence of operations
\begin{align*}
    \Gamma_{f} &= \sigma([h_{t-1}, x_{t}]W_{f} + b_{f}) \tag*{\text{forget gate}}\\
    \Gamma_{i} &= \sigma([h_{t-1}, x_{t}]W_{i} + b_{i}) \tag*{\text{input gate}}\\
    C_{t}^{\prime} &= tanh([h_{t-1}, x_{t}]W_{c} + b_{c}) \tag*{\text{cell state candidate}}\\
    C_{t} &= \Gamma_{f} \odot C_{t-1} + \Gamma_{i} \odot C_{t}^{\prime} \tag*{\text{new cell state}}\\
    \Gamma_{o} &= \sigma([h_{t-1}, x_{t}]W_{o} + b_{o}) \tag*{\text{output gate}}\\
    h_{t} &= \gamma_{o} \odot tanh(C_{t}) \tag*{\text{next hidden state}}
\end{align*}
\end{document}