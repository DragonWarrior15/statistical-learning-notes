\documentclass[../../deep_learning_notes.tex]{subfiles}
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Gradients for Common Activation Functions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Sigmoid}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{Single Data Point}
Let $z$ be the output of model, and $p$ be obtained after the sigmoid operation
\begin{align*}
    p &= \sigma(z) = \frac{1}{1 + exp(-z)}\\
    \frac{dp}{dz} &= \frac{(1 + exp(-z))(0) - (1)(-exp(-z))}{(1 + exp(-z))^{2}} = \frac{1}{1 + exp(-z)} \bigg(1 - \frac{1}{1 + exp(-z)} \bigg)\\
    &= p(1-p)
\end{align*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{Multiple Data Points}
Let $z,p \in \mathbb{R}^{n}$. Since this is an element-wise transformation, we leverage the derivation in \eqref{eq:backprop_2} 
\begin{align*}
    \frac{dL}{dz} &= \frac{dL}{dp}\odot p^{T}\odot (1-p)^{T}
\end{align*}

\subsection{Softmax}
\subsubsection*{Single Data Point}
Let $z \in \mathbb{R}^{k}$ be the $k$ outputs from the last layer of a neural network. To convert these real numbers to probabilities, we use the softmax function
\begin{align*}
    s &= \bigg( \frac{exp(z_{1})}{\sum_{i=1}^{k}\exp(z_{i})}, \frac{exp(z_{2})}{\sum_{i=1}^{k}\exp(z_{i})}, \ldots, \frac{exp(z_{k})}{\sum_{i=1}^{k}\exp(z_{i})} \bigg)^{T}
\end{align*}
where $s$ and $z$ are vectors with the same dimensions. Thus the derivative will be a matrix
\begin{align*}
    \frac{ds}{dz} &= \begin{bmatrix}
        \frac{\partial s_{1}}{\partial x_{1}}, &\cdots &\frac{\partial s_{1}}{\partial x_{k}}\\
        \vdots &\ddots &\vdots\\
        \frac{\partial s_{k}}{\partial x_{1}}, &\cdots &\frac{\partial s_{k}}{\partial x_{k}}\\
    \end{bmatrix}\\
    \frac{ds_{j}}{dz_{i}} &= \frac{d}{dz_{i}} \frac{exp(z_{j})}{\sum_{i=1}^{k}\exp(z_{i})}
    = \frac{-exp(z_{j})exp(z_{i})}{\big( \sum_{i=1}^{k}\exp(z_{i}) \big)^{2}}\\
    &= -s_{j}s_{i} \numberthiseqn\label{eq:activation_1}\\
    \frac{ds_{i}}{dz_{i}} &= \frac{d}{dz_{i}} \frac{exp(z_{i})}{\sum_{i=1}^{k}\exp(z_{i})}
    = \frac{(\sum_{i=1}^{k}\exp(z_{i}))exp(z_{i}) - exp(z_{i})(exp(z_{i}))}{\big( \sum_{i=1}^{k}\exp(z_{i}) \big)^{2}}\\
    &= \frac{exp(z_{i})}{\sum_{i=1}^{k}\exp(z_{i})} \bigg(1 -  \frac{exp(z_{i})}{\sum_{i=1}^{k}\exp(z_{i})}\bigg)\\
    &= s_{i}(1 - s_{i}) = s_{i} - s_{i}^{2} \numberthiseqn\label{eq:activation_2}\\
\end{align*}
Thus the off diagonal entries are simply products of the $y$ values at the corresponding indices. We form one matrix consisting of such values. For the diagonal entries, some additional manipulation is needed which is obtained by diagonal matrix consisting of $y$.
\begin{align*}
    \frac{ds}{dz} = diag(s) - s^{T}s = J
\end{align*}
where $J$ is the Jacobian matrix. Next, let's calculate the gradient
\begin{align*}
    \frac{dL}{dz} &= \frac{dL}{ds} \frac{ds}{dz}\\
    &= \bigg( \frac{dL}{ds_{1}}, \ldots, \frac{dL}{ds_{k}} \bigg) J\\
    \frac{dL}{dz_{i}} &= s_{i}(1-s_{i})\frac{dL}{ds_{i}} - \sum_{j=1, j\neq i}^{k}s_{i}s_{j}\frac{dL}{ds_{j}}\\
    &= s_{i}\frac{dL}{ds_{i}} - s_{i} \bigg( \sum_{j=1}^{k}\frac{dL}{ds_{j}}s_{j} \bigg)
\end{align*}
We have a common calculation across all gradients
\begin{align*}
    \sum_{j=1}^{k}\frac{dL}{ds_{j}}s_{j} = \frac{dL}{ds}s = c
\end{align*}
since $dl/ds$ is shape $1 \times k$ and $s$ is $k \times 1$. The gradient simplifies to
\begin{align*}
    \frac{dL}{dz_{i}} &= s_{i}\frac{dL}{ds_{i}} - s_{i}c = s_{i}(\frac{dL}{ds_{i}} - c)\\
    \frac{dL}{dz} &= \bigg(\frac{dL}{ds} - c \bigg) \odot s^{T}\\
    &= \bigg( \frac{dL}{ds} - \frac{dL}{ds}s \bigg) \odot s^{T}
\end{align*}
which involves far less calculations than the original formula involving the Jacobian.

\subsubsection*{Multiple Data Points}
In the case of multiple data points, $z, s \in \mathbb{R}^{N \times k}$ wherer $N$ is the total number of data points. In this case, we are talking of derivative of matrix over matrix which is a matrix of matrices of shape $N \times k \times N \times k$
\begin{align*}
    \frac{ds}{dz} &= \begin{bmatrix}
        \frac{ds_{11}}{dz} &\cdots &\frac{ds_{1k}}{dz}\\
        \vdots &\ddots &\vdots\\
        \frac{ds_{N1}}{dz} &\cdots &\frac{ds_{Nk}}{dz}\\
    \end{bmatrix}\\
    \frac{ds_{ij}}{dz} &= \begin{bmatrix}
        \frac{ds_{ij}}{dz_{11}} &\cdots &\frac{ds_{ij}}{dz_{1k}}\\
        \vdots &\ddots &\vdots\\
        \frac{ds_{ij}}{dz_{k1}} &\cdots &\frac{ds_{ij}}{dz_{nk}}\\
    \end{bmatrix}\\
    \frac{ds_{ij}}{dz_{lk}} &= 0 \quad i \neq l\\
    \frac{ds_{ij}}{dz_{ik}} &= \frac{d}{dz_{ik}} \frac{exp(z_{ij})}{\sum_{l=1}^{k}z_{il}} \quad j \neq k\\
    &= \frac{(\sum_{l=1}^{k}z_{il})(0) - exp(z_{ij})exp(z_{il})}{\big( (\sum_{l=1}^{k}z_{il})^{2} \big)}
    = -s_{ij}s_{il}\\
    \frac{ds_{ij}}{dz_{ij}} &= \frac{d}{dz_{ij}} \frac{exp(z_{ij})}{\sum_{l=1}^{k}z_{il}}
    = s_{ij}(1 - s_{ij})
\end{align*}
For $ds_{ij}/dz$, only the $i^{th}$ row is populated and rest all entries are zeros. Hence, when working with batches of $N$ examples, it is often the case that the complete matrix is reduced to the size  $N \times p \times p$ by summing along the third axis or the rows of $ds_{ij}/dz$. This reduces redundancy and removes all the entries of zeros.\newline

Using the Jacobian above is difficult to derive the gradients. We proceed in a manner similar to \eqref{eq:backprop_2} to calculate the gradient
\begin{align*}
    \frac{dL}{dz} &= \frac{dL}{ds} \frac{ds}{dz}\\
    \frac{dL}{dz_{ij}} &= \sum_{l=1}^{k} \frac{dL}{ds_{il}} \frac{ds_{il}}{dz_{ij}}\\
    &= \frac{dL}{ds_{ij}}s_{ij}(1-s_{ij}) - \sum_{l=1, l\neq j}^{k} \frac{dL}{ds_{il}}s_{il}s_{ij} \quad \text{from \eqref{eq:activation_1} and \eqref{eq:activation_2}}\\
    &= \frac{dL}{ds_{ij}}s_{ij} - s_{ij}\sum_{l=1}^{k}\frac{dL}{ds_{il}}s_{il}
    = \frac{dL}{ds_{ij}}s_{ij} - s_{ij}\frac{dL}{ds_{i}}s_{i}\\
    \frac{dL}{dz} &= \bigg( \frac{dL}{ds} - \bigg(\frac{dL}{ds} \odot s \bigg)\bm{1}_{k} \bigg) \odot s
\end{align*}
where $\bm{1}_{k}$ is a column vector of all ones. The second part of the equation involves broadcasting, since $s \in \mathbb{R}^{N \times k}$ and $\bigg(\frac{dL}{ds} \odot s \bigg)\bm{1}_{k} \in \mathbb{R}^{N \times 1}$.

\subsection{ReLU}
The defintion of \textbf{Rectified Linear Unit} is 
\begin{align*}
    z = max(0, x)
\end{align*}
and the derivative becomes
\begin{align*}
    \frac{dz}{dz} = \begin{cases}
        0 &\mbox{if $x < 0$}\\
        1 &\mbox{otherwise}
    \end{cases}
\end{align*}
Since the operation is element-wise, we can calculate the gradient as,
\begin{align*}
    z &= max(0, x)\\
    \frac{dL}{dx} &= \frac{dL}{dz} \frac{dz}{dx}\\
    &= \frac{dL}{dz} \odot (x > 0)\\
    (x > 0)_{ij} &= \begin{cases}
        0 &\mbox{if $x_{ij} < 0$}\\
        1 &\mbox{otherwise}
    \end{cases}
\end{align*}
where $x$ of shape $N \times k$.

\subsection{tanh}
The defintion of \textbf{tanh} is 
\begin{align*}
    z = tanh(x) = \frac{exp(x) - exp(-x)}{exp(x) + exp(-x)} = \frac{exp(2x) - 1}{exp(2x) - 1}
\end{align*}
and the derivative becomes
\begin{align*}
    \frac{dz}{dz} = \frac{(exp(x) + exp(-x))^{2} - (exp(x) - exp(-x))^{2}}{(exp(x) + exp(-x))^{2}} = 1 - tanh^{2}(x)
\end{align*}
Since the operation is element-wise, we can calculate the gradient as,
\begin{align*}
    z &= tanh(x)\\
    \frac{dL}{dX} &= \frac{dL}{dz} \odot (1 - tanh^{2}(x)) = \frac{dL}{dz} \odot (1 - z^{2})
\end{align*}
where $x \in \mathbb{R}^{N \times k}$
\end{document}